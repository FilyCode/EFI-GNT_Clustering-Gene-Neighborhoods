{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Gene Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set OMP_NUM_THREADS to 12 for SciPy/NumPy internal multi-threading.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Ensure multiprocessing is imported for os.cpu_count()\n",
    "import multiprocessing \n",
    "from threadpoolctl import threadpool_limits \n",
    "\n",
    "# Set OMP_NUM_THREADS as early as possible.\n",
    "# This variable primarily controls internal threading for numerical libraries like OpenBLAS/MKL\n",
    "# used by NumPy/SciPy.\n",
    "# When using joblib with n_jobs > 1 (process-based parallelism), each child process\n",
    "# will inherit this setting. However, to avoid oversubscription and ensure optimal performance\n",
    "# when a single joblib process performs an operation that *could* be multi-threaded itself\n",
    "# (like sparse matrix dot products), we will use `threadpool_limits` within the joblib task.\n",
    "# So, setting it globally is fine, but `threadpool_limits` will override it locally.\n",
    "num_logical_cores = os.cpu_count()\n",
    "if num_logical_cores:\n",
    "    # For OMP_NUM_THREADS, typically use all logical cores.\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_logical_cores) \n",
    "    print(f\"Set OMP_NUM_THREADS to {num_logical_cores} for SciPy/NumPy internal multi-threading.\")\n",
    "else:\n",
    "    print(\"Could not detect CPU count. OMP_NUM_THREADS not explicitly set.\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import string\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from joblib import Parallel, delayed \n",
    "from openpyxl import load_workbook \n",
    "\n",
    "\n",
    "# --- Configuration for the SQLite DB ---\n",
    "# EXCEL_FILE_PATH will be set in the __main__ block for convenience\n",
    "\n",
    "# Column names in the Excel sheets\n",
    "EXCEL_COL_LENGTH = '长度(aa)'\n",
    "EXCEL_COL_PFAM = 'pfam'\n",
    "EXCEL_COL_HIT_GENE = 'hit_gene'\n",
    "EXCEL_HIT_GENE_MARKER = 'yes' # Value indicating the hit gene\n",
    "\n",
    "# Keywords in Excel rows that signify the end of data for a neighborhood\n",
    "EXCEL_STOP_KEYWORDS = frozenset(['LOCUS', 'CDS', 'FEATURES'])\n",
    "\n",
    "# General Constants (adapted for Excel context)\n",
    "COL_NEIGHBORHOOD_ID = 'organism' # Maps to Excel sheet name\n",
    "COL_GENE_ID = 'id' # Maps to the first data column in Excel sheet (e.g., 'ctg5-78')\n",
    "\n",
    "HIT_GENE_WEIGHT_FACTOR = 10 # Factor by which hit gene features are \"copied\" for emphasis\n",
    "DIRECT_NEIGHBOR_WEIGHT_FACTOR = 3 # Factor for direct neighbor domain features\n",
    "\n",
    "SAVE_PLOTS = True # Set to True to save plots to files\n",
    "OUTPUT_DIR = 'gnn_cluster_plots_PEP' # Directory to save plots\n",
    "REPORT_FILENAME_BASE = 'gnn_clustering_report' # Base name, will append info dynamically\n",
    "OUTPUT_FORMATS = ['pdf'] # ['svg', 'png', 'pdf'] # List of formats to save plots in\n",
    "DPI = 300 # Dots per inch for raster formats like 'png'\n",
    "HIGHLIGHT_COLOR = 'red' # Color for the original input sequence's leaf label\n",
    "\n",
    "# Dynamic plot sizing parameters\n",
    "MIN_PLOT_HEIGHT = 8 # Minimum height of the plot in inches\n",
    "HEIGHT_PER_LEAF = 0.05 # Adjust this value (e.g., 0.2 to 0.5) to compress/stretch the Y-axis\n",
    "MAX_PLOT_HEIGHT = 40 # Maximum height to prevent excessively tall plots\n",
    "\n",
    "MIN_PLOT_WIDTH = 10 # Minimum width of the plot in inches\n",
    "WIDTH_PER_LEAF = 0.3 # Adjust this value (e.g., 0.3 to 0.7) to compress/stretch the X-axis (more horizontal space for branches)\n",
    "MAX_PLOT_WIDTH = 150 # Maximum width to prevent excessively wide plots\n",
    "\n",
    "# Configuration for collapsing similar neighborhoods\n",
    "COLLAPSE_IDENTICAL_NEIGHBORHOODS = True # Set to True to enable collapsing\n",
    "COLLAPSE_CORE_SIMILARITY_THRESHOLD = 0.0 # Strict similarity for hit gene + direct neighbors. Usually 0.0 for exact matches.\n",
    "COLLAPSE_FULL_NEIGHBORHOOD_SIMILARITY_THRESHOLD = 0.3 # Similarity for the entire neighborhood. e.g., 0.3 for 70% similarity.\n",
    "\n",
    "# Define a constant for the minimum number of samples to trigger parallel pdist\n",
    "MIN_ITEMS_FOR_PARALLEL_PROCESSING = 20 # Adjust this value as needed\n",
    "\n",
    "# Constants for PFAM cleaning\n",
    "PFAM_PREFIX = \"Pfam:\"\n",
    "UNINFORMATIVE_PFAM_TERMS = frozenset(['unknown', '-', '无', '', 'none', 'null', 'low complexity', 'Pfam', 'pfam']) \n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def parse_pfam_string(pfam_str, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Parses a string from the 'pfam' column of the Excel,\n",
    "    applying cleaning rules and a prefix.\n",
    "    \"\"\"\n",
    "    if not isinstance(pfam_str, str) or pd.isna(pfam_str):\n",
    "        return set()\n",
    "\n",
    "    initial_clean_str = str(pfam_str).strip() # Ensure it's a string type before processing\n",
    "\n",
    "    # Check for uninformative terms AFTER stripping\n",
    "    if initial_clean_str.lower() in UNINFORMATIVE_PFAM_TERMS:\n",
    "        return set()\n",
    "\n",
    "    features = set()\n",
    "    # Split by comma (,), semicolon (;), or Chinese comma (，)\n",
    "    parts = [p.strip() for p in re.split(r'[,;，]', initial_clean_str) if p.strip()]\n",
    "\n",
    "    for part in parts:\n",
    "        current_part = part.strip() # Ensure individual part is stripped\n",
    "\n",
    "        # --- FIX: Apply 'Pfam:' prefix removal to each individual part ---\n",
    "        if current_part.lower().startswith(PFAM_PREFIX.lower()):\n",
    "            current_part = current_part[len(PFAM_PREFIX):].strip()\n",
    "\n",
    "        # After prefix removal, re-check for uninformative parts\n",
    "        if current_part.lower() in UNINFORMATIVE_PFAM_TERMS:\n",
    "            continue\n",
    "        \n",
    "        # Collapse multiple spaces, convert to lowercase for consistency\n",
    "        clean_part = re.sub(r'\\s+', ' ', current_part).lower().strip()\n",
    "\n",
    "        if clean_part: # Ensure it's not empty after cleaning\n",
    "            features.add(f\"{prefix}{clean_part}\")\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_excel_gene_neighborhoods(excel_path, report_file=None):\n",
    "    \"\"\"\n",
    "    Reads gene neighborhood data from an Excel file, where each sheet is a neighborhood.\n",
    "    Extracts data for `长度(aa)`, `pfam`, `hit_gene` columns and up to stop keywords.\n",
    "    \"\"\"\n",
    "    def _write_and_print(text):\n",
    "        print(text)\n",
    "        if report_file:\n",
    "            report_file.write(text + '\\n')\n",
    "\n",
    "    _write_and_print(f\"Reading Excel file: {excel_path}\")\n",
    "    excel_data = {} # Dictionary to store DataFrames, key is sheet name\n",
    "\n",
    "    try:\n",
    "        workbook = load_workbook(excel_path, data_only=True) # data_only=True reads cell values, not formulas\n",
    "\n",
    "        for sheet_name in tqdm(workbook.sheetnames, desc=\"Processing Excel sheets\"):\n",
    "            _write_and_print(f\"  Processing sheet: {sheet_name}\")\n",
    "            worksheet = workbook[sheet_name]\n",
    "            \n",
    "            header_row_index = -1\n",
    "            header_cols = {} # To store column index for 'gene_id', '长度(aa)', 'pfam', 'hit_gene'\n",
    "\n",
    "            # Search for header row\n",
    "            for r_idx, row in enumerate(worksheet.iter_rows()):\n",
    "                current_row_values = [cell.value for cell in row]\n",
    "                # Check for header names (case-insensitive, strip whitespace)\n",
    "                lower_case_values = [str(v).strip().lower() if v is not None else '' for v in current_row_values]\n",
    "                \n",
    "                # Check if all required headers are present\n",
    "                if all(col_name.lower() in lower_case_values for col_name in [EXCEL_COL_LENGTH, EXCEL_COL_PFAM, EXCEL_COL_HIT_GENE]):\n",
    "                    header_row_index = r_idx + 1 # openpyxl rows are 1-indexed\n",
    "                    \n",
    "                    # Map required column names to their 0-indexed column positions\n",
    "                    for i, val in enumerate(lower_case_values):\n",
    "                        if val == EXCEL_COL_LENGTH.lower():\n",
    "                            header_cols[EXCEL_COL_LENGTH] = i\n",
    "                        elif val == EXCEL_COL_PFAM.lower():\n",
    "                            header_cols[EXCEL_COL_PFAM] = i\n",
    "                        elif val == EXCEL_COL_HIT_GENE.lower():\n",
    "                            header_cols[EXCEL_COL_HIT_GENE] = i\n",
    "                    \n",
    "                    # Assume gene ID column is always the first column (index 0)\n",
    "                    # Its header might be empty, as seen in examples.\n",
    "                    header_cols['gene_id_col'] = 0\n",
    "                    \n",
    "                    break # Header row found, stop searching\n",
    "            \n",
    "            if header_row_index == -1:\n",
    "                _write_and_print(f\"  Warning: Valid header row not found in sheet '{sheet_name}'. Skipping sheet.\")\n",
    "                continue\n",
    "\n",
    "            data_rows = []\n",
    "            gene_idx_counter = 0 # To assign unique identifiers within a neighborhood if gene IDs are missing or duplicate\n",
    "            for r_idx in range(header_row_index, worksheet.max_row + 1):\n",
    "                row_values = [cell.value for cell in worksheet[r_idx]]\n",
    "                \n",
    "                # Check for stop keywords or completely empty row\n",
    "                if not any(v is not None and str(v).strip() != '' for v in row_values) or \\\n",
    "                   any(str(v).strip().upper() in EXCEL_STOP_KEYWORDS for v in row_values if v is not None):\n",
    "                    break # Stop reading data for this sheet\n",
    "                \n",
    "                # Extract data based on identified column indices\n",
    "                gene_id_raw = row_values[header_cols['gene_id_col']] if 'gene_id_col' in header_cols and header_cols['gene_id_col'] < len(row_values) else None\n",
    "                \n",
    "                # Assign a robust gene ID: use existing, or construct if missing\n",
    "                gene_id = str(gene_id_raw) if gene_id_raw is not None and str(gene_id_raw).strip() != '' else f\"{sheet_name}_gene_{gene_idx_counter}\"\n",
    "\n",
    "                # Only attempt to get values if the column index is within row_values bounds\n",
    "                length = row_values[header_cols[EXCEL_COL_LENGTH]] if EXCEL_COL_LENGTH in header_cols and header_cols[EXCEL_COL_LENGTH] < len(row_values) else None\n",
    "                pfam = row_values[header_cols[EXCEL_COL_PFAM]] if EXCEL_COL_PFAM in header_cols and header_cols[EXCEL_COL_PFAM] < len(row_values) else None\n",
    "                hit_gene = row_values[header_cols[EXCEL_COL_HIT_GENE]] if EXCEL_COL_HIT_GENE in header_cols and header_cols[EXCEL_COL_HIT_GENE] < len(row_values) else None\n",
    "\n",
    "                # Only include rows that have a meaningful gene_id and pfam data\n",
    "                if gene_id is not None and str(gene_id).strip() != '' and (pfam is not None and str(pfam).strip() != ''):\n",
    "                    data_rows.append({\n",
    "                        COL_GENE_ID: gene_id,\n",
    "                        EXCEL_COL_LENGTH: length,\n",
    "                        EXCEL_COL_PFAM: pfam,\n",
    "                        EXCEL_COL_HIT_GENE: hit_gene\n",
    "                    })\n",
    "                gene_idx_counter += 1\n",
    "\n",
    "            if data_rows:\n",
    "                excel_data[sheet_name] = pd.DataFrame(data_rows)\n",
    "            else:\n",
    "                _write_and_print(f\"  Warning: No valid data rows found in sheet '{sheet_name}' after header. Skipping sheet.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        _write_and_print(f\"Error reading Excel file {excel_path}: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    _write_and_print(f\"Successfully read data from {len(excel_data)} sheets.\")\n",
    "    return excel_data\n",
    "\n",
    "\n",
    "def extract_features_from_excel_gene_row(gene_row_dict, current_weight_factor=1, base_prefix=\"N_\"):\n",
    "    \"\"\"\n",
    "    Extracts features (PFAM terms) from a single gene row dictionary (from Excel data),\n",
    "    applying a base prefix and duplicating features by current_weight_factor.\n",
    "    It uses the specialized parse_pfam_string for cleaning.\n",
    "    \"\"\"\n",
    "    features_set = set()\n",
    "    raw_features = set()\n",
    "\n",
    "    # Only PFAM is available as a functional description from the Excel format\n",
    "    pfam_ids = gene_row_dict.get(EXCEL_COL_PFAM)\n",
    "    raw_features.update(parse_pfam_string(pfam_ids))\n",
    "    \n",
    "    if current_weight_factor > 1:\n",
    "        for feature in raw_features:\n",
    "            for i in range(current_weight_factor):\n",
    "                features_set.add(f\"{base_prefix}{feature}_w{i}\") \n",
    "    else: \n",
    "        for feature in raw_features:\n",
    "            features_set.add(f\"{base_prefix}{feature}\")\n",
    "\n",
    "    return features_set\n",
    "\n",
    "\n",
    "def _plot_dendrogram(linked, neighborhood_ids_subset, labels_map, distance_threshold, \n",
    "                     plot_title_base, label_type, original_input_sequence_id,\n",
    "                     save_plots, output_dir, output_formats, dpi, \n",
    "                     min_plot_height, height_per_leaf, max_plot_height,\n",
    "                     min_plot_width, width_per_leaf, max_plot_width,\n",
    "                     report_file=None):\n",
    "    \"\"\"\n",
    "    Helper function to generate a single dendrogram plot.\n",
    "    \"\"\"\n",
    "    # An internal helper function for consistent logging\n",
    "    def _write_and_print_internal(text):\n",
    "        # Only print to console for plots as it can be very verbose\n",
    "        # and only save to report if it's a critical message.\n",
    "        # Plotting messages are now controlled outside this function.\n",
    "        if report_file:\n",
    "            report_file.write(text + '\\n')\n",
    "    \n",
    "    fig_title = f\"{plot_title_base} ({label_type.capitalize()} Labels)\"\n",
    "    \n",
    "    labels_to_use = []\n",
    "    # Store the hit_id for each label in the order it will be plotted, to match with xticklabels\n",
    "    accession_ids_for_labels = []\n",
    "    for nh_id in neighborhood_ids_subset:\n",
    "        organism_name, hit_id_internal, ssn_cluster_id, accession_id, _ = labels_map.get(nh_id, ('Unknown', 'Unknown', None, 'Unknown', None))\n",
    "\n",
    "        if label_type == 'organism':\n",
    "            labels_to_use.append(organism_name.rstrip('.'))\n",
    "        elif label_type == 'id': # This now means 'accession'\n",
    "            labels_to_use.append(accession_id) # Use accession_id for 'id' labels\n",
    "        else:\n",
    "            labels_to_use.append(nh_id) # Fallback, should not be hit\n",
    "        accession_ids_for_labels.append(accession_id)\n",
    "\n",
    "    # Figure size calculation\n",
    "    num_leaves = len(neighborhood_ids_subset)\n",
    "    \n",
    "    calculated_height = max(min_plot_height, num_leaves * height_per_leaf)\n",
    "    final_height = min(calculated_height, max_plot_height)\n",
    "    \n",
    "    calculated_width = max(min_plot_width, num_leaves * width_per_leaf)\n",
    "    final_width = min(calculated_width, max_plot_width)\n",
    "    \n",
    "    plt.figure(figsize=(final_width, final_height)) # Use dynamic width and height\n",
    "    \n",
    "    dendrogram(linked,\n",
    "               orientation='top',\n",
    "               labels=labels_to_use,\n",
    "               distance_sort='descending',\n",
    "               show_leaf_counts=True)\n",
    "    \n",
    "    plt.title(fig_title)\n",
    "    plt.xlabel('Gene Neighborhood (Labeled by ' + ('Accession' if label_type == 'id' else label_type.capitalize()) + ')')     # Y-axis label is now fixed to Linear Scale\n",
    "    plt.ylabel(f'Jaccard Distance') \n",
    "    plt.axhline(y=distance_threshold, color='r', linestyle='--', label=f'Cut-off at {distance_threshold}')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.gca()    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Color specific leaf label\n",
    "    if original_input_sequence_id:\n",
    "        for i, tick_label in enumerate(ax.get_xticklabels()):\n",
    "            if accession_ids_for_labels[i] == original_input_sequence_id:\n",
    "                tick_label.set_color(HIGHLIGHT_COLOR)\n",
    "                tick_label.set_weight('bold') # Make it bold for more prominence\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_plots:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        clean_plot_title_base = re.sub(r'[^\\w\\s-]', '', plot_title_base).replace(' ', '_')\n",
    "        base_filename = f\"{clean_plot_title_base}_{label_type}_labels\"\n",
    "        \n",
    "        # Save in multiple formats ---\n",
    "        for fmt in output_formats:\n",
    "            full_filename = f\"{base_filename}.{fmt}\"\n",
    "            plt.savefig(os.path.join(output_dir, full_filename), format=fmt, dpi=dpi)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def parallel_pdist_jaccard(feature_matrix, num_cores=-1):\n",
    "    \"\"\"\n",
    "    Calculates the condensed Jaccard distance matrix.\n",
    "    Uses joblib parallelization only if n_samples > MIN_ITEMS_FOR_PARALLEL_PROCESSING.\n",
    "    Otherwise, it runs sequentially.\n",
    "    \"\"\"\n",
    "    if not isinstance(feature_matrix, sp.csr_matrix):\n",
    "        if isinstance(feature_matrix, (sp.csc_matrix, sp.lil_matrix, sp.coo_matrix)):\n",
    "            feature_matrix = feature_matrix.tocsr()\n",
    "        else:\n",
    "            raise TypeError(\"Input feature_matrix must be a SciPy sparse matrix (preferably CSR).\")\n",
    "\n",
    "    n_samples = feature_matrix.shape[0]\n",
    "    if n_samples <= 1:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Determine the number of cores to use, with a fallback to 1 if detection fails\n",
    "    if num_cores == -1:\n",
    "        detected_cores = os.cpu_count()\n",
    "        num_cores = detected_cores if detected_cores is not None and detected_cores > 0 else 1\n",
    "    elif num_cores == 0: # Treat 0 as explicit sequential execution\n",
    "        num_cores = 1\n",
    "\n",
    "    # --- Pre-compute all feature sets once in the parent process ---\n",
    "    # This is always beneficial for performance regardless of parallelization,\n",
    "    # as it avoids repeated set construction from the sparse matrix.\n",
    "    # The tqdm desc is now more informative and reflects the context.\n",
    "    print(f\"  Pre-calculating {n_samples} feature sets...\")\n",
    "    set_precomputation_start = time.time()\n",
    "    feature_sets = [\n",
    "        set(feature_matrix.indices[feature_matrix.indptr[i]:feature_matrix.indptr[i+1]])\n",
    "        for i in tqdm(range(n_samples), desc=f\"  Pre-computing feature sets (N={n_samples})\", leave=False)\n",
    "    ]\n",
    "    print(f\"  Feature set pre-calculation took {time.time() - set_precomputation_start:.2f} seconds.\")\n",
    "\n",
    "    # Explicitly delete the sparse matrix after feature_sets are extracted.\n",
    "    del feature_matrix\n",
    "    gc.collect()\n",
    "\n",
    "    # Define the core distance calculation logic (for both sequential and parallel workers)\n",
    "    def _calculate_distances_from_sets(start_i, end_i, all_feature_sets_ref, n_samples_total_ref):\n",
    "        # IMPORTANT: Limit internal (BLAS/LAPACK) threading for each joblib process to 1.\n",
    "        with threadpool_limits(limits=1, user_api='blas'):\n",
    "            distances_chunk = []\n",
    "            \n",
    "            for i in range(start_i, end_i):\n",
    "                set_i = all_feature_sets_ref[i]\n",
    "                for j in range(i + 1, n_samples_total_ref):\n",
    "                    set_j = all_feature_sets_ref[j]\n",
    "\n",
    "                    intersection_size = len(set_i.intersection(set_j))\n",
    "                    union_size = len(set_i.union(set_j))\n",
    "\n",
    "                    if union_size == 0:\n",
    "                        distances_chunk.append(0.0)\n",
    "                    else:\n",
    "                        distances_chunk.append(1.0 - (intersection_size / union_size))\n",
    "            return distances_chunk\n",
    "\n",
    "    total_i_iterations = n_samples - 1\n",
    "    if total_i_iterations <= 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # --- Conditional Parallelization Logic ---\n",
    "    if n_samples < MIN_ITEMS_FOR_PARALLEL_PROCESSING or num_cores == 1:\n",
    "        print(f\"  Running Jaccard distance sequentially for N={n_samples} (below parallel threshold or num_cores=1).\")\n",
    "        results = [_calculate_distances_from_sets(0, total_i_iterations, feature_sets, n_samples)]\n",
    "    else:\n",
    "        print(f\"  Running Jaccard distance in parallel for N={n_samples} using {num_cores} cores.\")\n",
    "        i_ranges_for_tasks = []\n",
    "        # Create more granular tasks for better load balancing, but not excessively so for small N\n",
    "        num_tasks = min(total_i_iterations, num_cores * 4) # Up to 4 tasks per core, but no more than i_iterations\n",
    "        if num_tasks == 0: return np.array([]) # Safety check for very small N\n",
    "        \n",
    "        chunk_size_for_i = max(1, (total_i_iterations + num_tasks - 1) // num_tasks)\n",
    "        \n",
    "        for k in range(0, total_i_iterations, chunk_size_for_i):\n",
    "            start_i = k\n",
    "            end_i = min(k + chunk_size_for_i, total_i_iterations)\n",
    "            i_ranges_for_tasks.append((start_i, end_i))\n",
    "            \n",
    "        tasks = [\n",
    "            delayed(_calculate_distances_from_sets)(start_i_chunk, end_i_chunk, feature_sets, n_samples)\n",
    "            for start_i_chunk, end_i_chunk in i_ranges_for_tasks\n",
    "        ]\n",
    "        \n",
    "        results = Parallel(n_jobs=num_cores, backend=\"loky\", verbose=0)(tasks)\n",
    "\n",
    "    condensed_distances = np.concatenate(results)\n",
    "    return np.array(condensed_distances)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _perform_collapsing(all_neighborhood_features, full_neighborhood_labels_map, \n",
    "                        core_neighborhood_features,\n",
    "                        collapse_core_similarity_threshold, collapse_full_neighborhood_similarity_threshold,\n",
    "                        output_prefix=\"\", report_file=None, parallelize_pdist=False):\n",
    "    \"\"\"\n",
    "    Performs a two-stage collapsing of similar neighborhoods.\n",
    "    Stage 1: Group by core (hit + direct neighbors) features.\n",
    "    Stage 2: Within these groups, sub-group by full neighborhood features.\n",
    "\n",
    "    Returns: (final_neighborhood_features, final_neighborhood_labels_map, collapsed_groups_report)\n",
    "    \"\"\"\n",
    "    # An internal helper function for consistent logging\n",
    "    def _write_and_print_internal(text):\n",
    "        print(text)\n",
    "        if report_file:\n",
    "            report_file.write(text + '\\n')\n",
    "\n",
    "    _write_and_print_internal(f\"{output_prefix}  Starting two-stage collapsing (Core Thr: {collapse_core_similarity_threshold}, Full Thr: {collapse_full_neighborhood_similarity_threshold}).\")\n",
    "    \n",
    "    collapsing_overall_start = time.time()\n",
    "\n",
    "    collapsed_groups_report = {}\n",
    "    \n",
    "    # --- Stage 1: Group by CORE features (hit + direct neighbors) ---\n",
    "    stage1_start = time.time()\n",
    "    core_labels_ordered = sorted(list(core_neighborhood_features.keys()))\n",
    "    if len(core_labels_ordered) < 2:\n",
    "        _write_and_print_internal(f\"{output_prefix}  Only {len(core_labels_ordered)} neighborhood(s) to process. Skipping collapsing.\")\n",
    "        return all_neighborhood_features, full_neighborhood_labels_map, collapsed_groups_report\n",
    "\n",
    "    core_vocabulary = sorted(list(set.union(*core_neighborhood_features.values())))\n",
    "    if not core_vocabulary:\n",
    "        _write_and_print_internal(f\"{output_prefix}  Warning: No core features found for collapsing. Skipping collapsing step.\")\n",
    "        return all_neighborhood_features, full_neighborhood_labels_map, collapsed_groups_report\n",
    "\n",
    "    _write_and_print_internal(f\"{output_prefix}  Stage 1: Building sparse matrix for {len(core_labels_ordered)} neighborhoods and {len(core_vocabulary)} core features...\")\n",
    "    matrix_build_start = time.time()\n",
    "    core_feature_to_idx = {feature: i for i, feature in enumerate(core_vocabulary)}\n",
    "    num_core_neighborhoods = len(core_labels_ordered)\n",
    "    num_core_features = len(core_vocabulary)\n",
    "\n",
    "    # Use LIL for efficient construction, then convert to CSR for computation\n",
    "    core_feature_vectors_lil = sp.lil_matrix((num_core_neighborhoods, num_core_features), dtype=np.int8) \n",
    "    for i, nh_label in enumerate(tqdm(core_labels_ordered, desc=f\"{output_prefix}  Stage 1: Populating core features\", leave=False)): # <-- MODIFIED: Added tqdm\n",
    "        for feature in core_neighborhood_features[nh_label]:\n",
    "            if feature in core_feature_to_idx: # Safety check\n",
    "                j = core_feature_to_idx[feature]\n",
    "                core_feature_vectors_lil[i, j] = 1\n",
    "    core_feature_vectors = core_feature_vectors_lil.tocsr() # Convert to CSR\n",
    "    _write_and_print_internal(f\"{output_prefix}  Stage 1: Sparse matrix built in {time.time() - matrix_build_start:.2f} seconds. Shape: {core_feature_vectors.shape}, NNZ: {core_feature_vectors.nnz}\") # <-- ADDED: Detailed report\n",
    "    gc.collect() # <-- ADDED: Garbage collection\n",
    "\n",
    "    if core_feature_vectors.shape[0] < 2: # Check again after creating vectors\n",
    "        _write_and_print_internal(f\"{output_prefix}  Only {core_feature_vectors.shape[0]} valid core feature vector(s). Skipping collapsing.\")\n",
    "        return all_neighborhood_features, full_neighborhood_labels_map, collapsed_groups_report\n",
    "\n",
    "    # Check for identical sparse vectors\n",
    "    if num_core_neighborhoods > 1 and all(\n",
    "        (core_feature_vectors[0] != core_feature_vectors[i]).nnz == 0 # Compare sparse rows\n",
    "        for i in range(1, num_core_neighborhoods)\n",
    "    ):\n",
    "        core_pre_clusters = {core_labels_ordered[i]: 1 for i in range(len(core_labels_ordered))}\n",
    "        _write_and_print_internal(f\"{output_prefix}  All core feature vectors are identical. Treating as one initial core group.\")\n",
    "    else:\n",
    "        _write_and_print_internal(f\"{output_prefix}  Stage 1: Calculating core distances using scipy.pdist...\") # <-- MODIFIED: Detailed report\n",
    "        distance_calc_start = time.time()\n",
    "        core_distances = parallel_pdist_jaccard(core_feature_vectors, num_cores=-1 if parallelize_pdist else 1)\n",
    "        _write_and_print_internal(f\"{output_prefix}  Stage 1: Core distance calculation took {time.time() - distance_calc_start:.2f} seconds.\")\n",
    "\n",
    "        _write_and_print_internal(f\"{output_prefix}  Stage 1: Performing linkage and clustering for core features...\") # <-- MODIFIED: Detailed report\n",
    "        linkage_start = time.time()\n",
    "        core_linked = linkage(core_distances, method='average')\n",
    "        _write_and_print_internal(f\"{output_prefix}  Stage 1: Linkage took {time.time() - linkage_start:.2f} seconds.\") # <-- ADDED: Detailed report\n",
    "        core_pre_clusters = fcluster(core_linked, collapse_core_similarity_threshold, criterion='distance')\n",
    "    \n",
    "    # Group neighborhoods by their initial core-feature-based cluster\n",
    "    initial_core_groups = defaultdict(list)\n",
    "    # The output of fcluster is an array of cluster IDs, map them back to labels_ordered\n",
    "    # Handling for when core_pre_clusters is already a dict (all identical) or an array\n",
    "    if isinstance(core_pre_clusters, np.ndarray): \n",
    "        for i, group_id in enumerate(core_pre_clusters):\n",
    "            initial_core_groups[group_id].append(core_labels_ordered[i])\n",
    "    else: \n",
    "        initial_core_groups = {1: core_labels_ordered} # Put all in one group (from the `if all(...)` block)\n",
    "    \n",
    "    _write_and_print_internal(f\"{output_prefix}  Stage 1: Grouped into {len(initial_core_groups)} initial core groups based on a threshold of {collapse_core_similarity_threshold}. Total stage 1 took {time.time() - stage1_start:.2f} seconds.\") # <-- ADDED: Detailed report\n",
    "\n",
    "    # Explicit memory cleanup for Stage 1 objects\n",
    "    del core_feature_vectors \n",
    "    if 'core_distances' in locals(): del core_distances \n",
    "    if 'core_linked' in locals(): del core_linked \n",
    "    gc.collect() # Garbage collection\n",
    "\n",
    "    # --- Stage 2: Sub-group by FULL neighborhood features within each core group ---\n",
    "    stage2_start = time.time()\n",
    "    \n",
    "    # We'll generate letter codes like A, B, ..., Z, AA, AB, ... for robustness\n",
    "    def generate_letter_code(index):\n",
    "        if index < 26:\n",
    "            return string.ascii_uppercase[index]\n",
    "        else:\n",
    "            first_char_idx = (index // 26) - 1\n",
    "            second_char_idx = index % 26\n",
    "            return f\"{string.ascii_uppercase[first_char_idx]}{string.ascii_uppercase[second_char_idx]}\"\n",
    "\n",
    "    _write_and_print_internal(f\"{output_prefix}  Stage 2: Processing {len(initial_core_groups)} core groups for full neighborhood similarity...\")\n",
    "\n",
    "    # --- Define the worker function for processing a chunk of core groups ---\n",
    "    def process_core_group_chunk(core_group_ids_chunk,\n",
    "                                 all_neighborhood_features_ref,\n",
    "                                 full_neighborhood_labels_map_ref,\n",
    "                                 collapse_full_neighborhood_similarity_threshold_ref,\n",
    "                                 parallelize_pdist_ref):\n",
    "        \n",
    "        results_to_aggregate = []\n",
    "        local_collapsed_total_count = 0\n",
    "        \n",
    "        # Determine internal parallelism for pdist calls *within this worker*\n",
    "        # This decision is based on parallelize_pdist_ref (if outer parallelization is on)\n",
    "        # and the MIN_ITEMS_FOR_PARALLEL_PROCESSING threshold.\n",
    "        # So, if parallelize_pdist_ref is True, then we allow the internal pdist calls to be parallel\n",
    "        # if they meet their own MIN_ITEMS_FOR_PARALLEL_PROCESSING threshold (which is now called MIN_ITEMS_FOR_PARALLEL_PROCESSING).\n",
    "        # Otherwise, internal pdist calls are sequential.\n",
    "        num_cores_for_internal_pdist = -1 if parallelize_pdist_ref else 1 \n",
    "\n",
    "        with threadpool_limits(limits=1, user_api='blas'):\n",
    "            for group_id in core_group_ids_chunk:\n",
    "                members_in_core_group = initial_core_groups[group_id]\n",
    "                \n",
    "                if len(members_in_core_group) < 2:\n",
    "                    member_label = members_in_core_group[0]\n",
    "                    results_to_aggregate.append((member_label,\n",
    "                                                 all_neighborhood_features_ref[member_label],\n",
    "                                                 full_neighborhood_labels_map_ref[member_label],\n",
    "                                                 None))\n",
    "                    continue\n",
    "\n",
    "                sub_group_vocabulary = sorted(list(set.union(*[all_neighborhood_features_ref[m] for m in members_in_core_group])))\n",
    "                if not sub_group_vocabulary:\n",
    "                    sub_group_assignments = {m: 1 for m in members_in_core_group}\n",
    "                else:\n",
    "                    sub_group_feature_to_idx = {feature: i for i, feature in enumerate(sub_group_vocabulary)}\n",
    "                    num_sub_group_neighborhoods = len(members_in_core_group)\n",
    "                    num_sub_group_features = len(sub_group_vocabulary)\n",
    "\n",
    "                    sub_group_feature_vectors_lil = sp.lil_matrix((num_sub_group_neighborhoods, num_sub_group_features), dtype=np.int8)\n",
    "                    for i, nh_label in enumerate(members_in_core_group):\n",
    "                        for feature in all_neighborhood_features_ref[nh_label]:\n",
    "                            if feature in sub_group_feature_to_idx:\n",
    "                                j = sub_group_feature_to_idx[feature]\n",
    "                                sub_group_feature_vectors_lil[i, j] = 1\n",
    "                    sub_group_feature_vectors = sub_group_feature_vectors_lil.tocsr()\n",
    "\n",
    "                    if num_sub_group_neighborhoods > 1 and all(\n",
    "                        (sub_group_feature_vectors[0] != sub_group_feature_vectors[i]).nnz == 0\n",
    "                        for i in range(1, num_sub_group_neighborhoods)\n",
    "                    ):\n",
    "                        sub_group_assignments = {members_in_core_group[i]: 1 for i in range(len(members_in_core_group))}\n",
    "                    else:\n",
    "                        sub_group_distances = parallel_pdist_jaccard(\n",
    "                            sub_group_feature_vectors,\n",
    "                            num_cores=num_cores_for_internal_pdist # Use the determined internal pdist cores\n",
    "                        )\n",
    "                        if sub_group_distances.size == 0 and num_sub_group_neighborhoods <= 1: # Added <=1 for robustness\n",
    "                            sub_group_assignments = {members_in_core_group[0]: 1}\n",
    "                        elif sub_group_distances.size == 0 and num_sub_group_neighborhoods > 1:\n",
    "                            sub_group_assignments = {m: 1 for m in members_in_core_group}\n",
    "                        else:\n",
    "                            sub_group_linked = linkage(sub_group_distances, method='average')\n",
    "                            sub_group_assignments = fcluster(sub_group_linked, collapse_full_neighborhood_similarity_threshold_ref, criterion='distance')\n",
    "                    \n",
    "                    del sub_group_feature_vectors\n",
    "                    if 'sub_group_distances' in locals(): del sub_group_distances\n",
    "                    if 'sub_group_linked' in locals(): del sub_group_linked\n",
    "                    gc.collect()\n",
    "\n",
    "                current_sub_groups = defaultdict(list)\n",
    "                if isinstance(sub_group_assignments, np.ndarray):\n",
    "                    for i, sub_cluster_id in enumerate(sub_group_assignments):\n",
    "                        current_sub_groups[sub_cluster_id].append(members_in_core_group[i])\n",
    "                else:\n",
    "                    current_sub_groups = {1: members_in_core_group}\n",
    "\n",
    "                for sub_cluster_id in sorted(current_sub_groups.keys()):\n",
    "                    collapsed_members = current_sub_groups[sub_cluster_id]\n",
    "                    \n",
    "                    if len(collapsed_members) > 1:\n",
    "                        local_collapsed_total_count += (len(collapsed_members) - 1)\n",
    "                        representative_label = collapsed_members[0]\n",
    "                        \n",
    "                        union_features = set()\n",
    "                        for member_label in collapsed_members:\n",
    "                            union_features.update(all_neighborhood_features_ref[member_label])\n",
    "\n",
    "                        results_to_aggregate.append((representative_label,\n",
    "                                                     union_features,\n",
    "                                                     full_neighborhood_labels_map_ref[representative_label],\n",
    "                                                     collapsed_members))\n",
    "                    else:\n",
    "                        member_label = collapsed_members[0]\n",
    "                        results_to_aggregate.append((member_label,\n",
    "                                                     all_neighborhood_features_ref[member_label],\n",
    "                                                     full_neighborhood_labels_map_ref[member_label],\n",
    "                                                     None))\n",
    "        return results_to_aggregate, local_collapsed_total_count\n",
    "\n",
    "\n",
    "    # Prepare arguments for parallel execution\n",
    "    all_core_group_ids = sorted(list(initial_core_groups.keys()))\n",
    "    num_total_core_groups = len(all_core_group_ids)\n",
    "\n",
    "    # Conditional Parallelization for Stage 2 Outer Loop \n",
    "    if num_total_core_groups < MIN_ITEMS_FOR_PARALLEL_PROCESSING or not parallelize_pdist:\n",
    "        _write_and_print_internal(f\"{output_prefix}  Stage 2: Running sequentially for {num_total_core_groups} core groups (below parallel threshold or parallelization disabled).\")\n",
    "        # Run sequentially\n",
    "        results_from_workers = [\n",
    "            process_core_group_chunk(\n",
    "                all_core_group_ids,\n",
    "                all_neighborhood_features,\n",
    "                full_neighborhood_labels_map,\n",
    "                collapse_full_neighborhood_similarity_threshold,\n",
    "                False # Explicitly tell inner pdist to run sequentially if outer is sequential\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        # Determine the number of cores to use for stage 2 parallelism\n",
    "        num_stage2_cores = os.cpu_count() if parallelize_pdist else 1\n",
    "        if num_stage2_cores <= 0: num_stage2_cores = 1\n",
    "\n",
    "        _write_and_print_internal(f\"{output_prefix}  Stage 2: Distributing {num_total_core_groups} core groups among {num_stage2_cores} workers (parallelized).\")\n",
    "\n",
    "        # Create chunks of core group IDs to distribute\n",
    "        chunk_size = max(1, num_total_core_groups // num_stage2_cores)\n",
    "        group_id_chunks = [all_core_group_ids[i:i + chunk_size] for i in range(0, num_total_core_groups, chunk_size)]\n",
    "        \n",
    "        results_from_workers = Parallel(n_jobs=num_stage2_cores, backend=\"loky\", verbose=100)(\n",
    "            delayed(process_core_group_chunk)(\n",
    "                chunk,\n",
    "                all_neighborhood_features,\n",
    "                full_neighborhood_labels_map,\n",
    "                collapse_full_neighborhood_similarity_threshold,\n",
    "                parallelize_pdist # Pass the overall parallelize_pdist flag\n",
    "            ) for chunk in group_id_chunks\n",
    "        )\n",
    "\n",
    "    # --- Aggregate results from workers in the main process ---\n",
    "    final_neighborhood_features = {}\n",
    "    final_neighborhood_labels_map = {}\n",
    "    collapsed_total_count = 0\n",
    "    unique_collapsed_group_counter = 0 # Single global counter\n",
    "\n",
    "    for worker_results, worker_collapsed_count in results_from_workers:\n",
    "        collapsed_total_count += worker_collapsed_count\n",
    "        for representative_label, features, original_labels_map_entry, collapsed_members in worker_results:\n",
    "            if collapsed_members is not None: # This was a collapsed group\n",
    "                letter_code = generate_letter_code(unique_collapsed_group_counter)\n",
    "                unique_collapsed_group_counter += 1\n",
    "                \n",
    "                orig_organism, orig_hit_id, orig_ssn_id, orig_accession, _ = original_labels_map_entry\n",
    "                final_neighborhood_labels_map[representative_label] = (orig_organism, orig_hit_id, orig_ssn_id, orig_accession, (len(collapsed_members), letter_code))\n",
    "                \n",
    "                collapsed_groups_report[letter_code] = {\n",
    "                    'representative': representative_label,\n",
    "                    'members': sorted(collapsed_members),\n",
    "                    'count': len(collapsed_members)\n",
    "                }\n",
    "                final_neighborhood_features[representative_label] = features\n",
    "            else: # Not a collapsed group, just an individual neighborhood\n",
    "                final_neighborhood_features[representative_label] = features\n",
    "                final_neighborhood_labels_map[representative_label] = original_labels_map_entry\n",
    "\n",
    "\n",
    "    if collapsed_total_count > 0:\n",
    "        _write_and_print_internal(f\"{output_prefix}  Collapsed a total of {collapsed_total_count} neighborhoods into {len(final_neighborhood_features)} unique entities after two stages. Stage 2 took {time.time() - stage2_start:.2f} seconds.\")\n",
    "    else:\n",
    "        _write_and_print_internal(f\"{output_prefix}  No neighborhoods were collapsed after two stages (or disabled). Stage 2 took {time.time() - stage2_start:.2f} seconds.\")\n",
    "\n",
    "    _write_and_print_internal(f\"{output_prefix}  Overall collapsing took {time.time() - collapsing_overall_start:.2f} seconds.\")\n",
    "    return final_neighborhood_features, final_neighborhood_labels_map, collapsed_groups_report\n",
    "\n",
    "\n",
    "def cluster_gene_neighborhoods_from_excel(\n",
    "    excel_path,\n",
    "    project_name,\n",
    "    hit_gene_weight_factor=HIT_GENE_WEIGHT_FACTOR,\n",
    "    direct_neighbor_weight_factor=DIRECT_NEIGHBOR_WEIGHT_FACTOR, \n",
    "    original_input_sequence_id=None, \n",
    "    distance_threshold=0.8,\n",
    "    plot_dendrogram=True,\n",
    "    save_plots=SAVE_PLOTS,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    output_formats=OUTPUT_FORMATS, \n",
    "    dpi=DPI,\n",
    "    min_plot_height=MIN_PLOT_HEIGHT, \n",
    "    height_per_leaf=HEIGHT_PER_LEAF, \n",
    "    max_plot_height=MAX_PLOT_HEIGHT,\n",
    "    min_plot_width=MIN_PLOT_WIDTH,\n",
    "    width_per_leaf=WIDTH_PER_LEAF,\n",
    "    max_plot_width=MAX_PLOT_WIDTH,\n",
    "    report_file_handle=None, \n",
    "    parallelize_pdist=False,\n",
    "    collapse_identical_neighborhoods=COLLAPSE_IDENTICAL_NEIGHBORHOODS,\n",
    "    collapse_core_similarity_threshold=COLLAPSE_CORE_SIMILARITY_THRESHOLD,\n",
    "    collapse_full_neighborhood_similarity_threshold=COLLAPSE_FULL_NEIGHBORHOOD_SIMILARITY_THRESHOLD\n",
    "):\n",
    "    \"\"\"\n",
    "    Clusters gene neighborhoods from an Excel file.\n",
    "    Each sheet is a gene neighborhood. Identifies hit gene and direct neighbors for weighting.\n",
    "    The clustering is performed across all neighborhoods in the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing (clusters_dict, final_neighborhood_labels_map, collapsed_groups_report).\n",
    "               clusters_dict is a dict where keys are 'All_Excel_Neighborhoods' \n",
    "               and values are dicts of cluster_id -> list of unique_neighborhood_labels (which might be representatives).\n",
    "               final_neighborhood_labels_map is a dict mapping unique_neighborhood_label to \n",
    "               (organism_name, hit_gene_id, ssn_cluster_id (dummy), accession_id (gene_id), collapsed_members_info).\n",
    "               collapsed_groups_report is a dict detailing the collapsed groups.\n",
    "    \"\"\"\n",
    "    def _write_and_print_internal(text):\n",
    "        print(text)\n",
    "        if report_file_handle:\n",
    "            report_file_handle.write(text + '\\n')\n",
    "\n",
    "    start_time_overall = time.time()\n",
    "    \n",
    "    # --- Step 1: Read all data from Excel ---\n",
    "    excel_neighborhood_data = read_excel_gene_neighborhoods(excel_path, report_file=report_file_handle)\n",
    "\n",
    "    if not excel_neighborhood_data:\n",
    "        _write_and_print_internal(\"No gene neighborhoods found in the Excel file. Exiting.\")\n",
    "        return {}, {}, {}\n",
    "\n",
    "    all_neighborhood_features = defaultdict(set)\n",
    "    core_neighborhood_features = defaultdict(set)\n",
    "    # Maps unique_neighborhood_label to (organism_name, hit_gene_id, ssn_cluster_id (dummy), accession_id (gene_id), collapsed_info)\n",
    "    full_neighborhood_labels_map = {} \n",
    "\n",
    "    _write_and_print_internal(f\"Processing features for {len(excel_neighborhood_data)} gene neighborhoods from Excel sheets...\")\n",
    "    feature_extraction_start_time = time.time()\n",
    "\n",
    "    # Loop through each sheet, which represents one gene neighborhood\n",
    "    for sheet_name, df_neighborhood in tqdm(excel_neighborhood_data.items(), desc=\"Extracting features from Excel neighborhoods\", unit=\"neighborhood\"):\n",
    "        \n",
    "        organism_name = sheet_name # Worksheet name serves as the organism/neighborhood ID\n",
    "        \n",
    "        # Identify the hit gene by the 'yes' marker\n",
    "        hit_gene_row_series = df_neighborhood[df_neighborhood[EXCEL_COL_HIT_GENE].astype(str).str.lower() == EXCEL_HIT_GENE_MARKER.lower()]\n",
    "        \n",
    "        if hit_gene_row_series.empty:\n",
    "            _write_and_print_internal(f\"  Warning: No 'hit_gene' marked 'yes' in sheet '{sheet_name}'. Skipping this neighborhood.\")\n",
    "            continue \n",
    "\n",
    "        # If multiple hit genes are marked 'yes', take the first one encountered in the DataFrame order\n",
    "        hit_gene_row = hit_gene_row_series.iloc[0].to_dict() \n",
    "        hit_gene_id = hit_gene_row[COL_GENE_ID]\n",
    "\n",
    "        # For the Excel data, there's no explicit accession ID or SSN cluster ID.\n",
    "        # We use the gene_id as the accession_id for highlighting purposes in plots.\n",
    "        # A dummy SSN cluster ID is used as all Excel neighborhoods are clustered together.\n",
    "        accession_id = hit_gene_id \n",
    "        ssn_cluster_id = 'Excel_Data_Cluster' \n",
    "\n",
    "        unique_neighborhood_label = f\"{organism_name}_{hit_gene_id}\"\n",
    "        \n",
    "        current_full_features = set()\n",
    "        current_core_features = set()\n",
    "\n",
    "        # Add features of the HIT gene itself with its special weight\n",
    "        hit_full_features = extract_features_from_excel_gene_row(\n",
    "            gene_row_dict=hit_gene_row, \n",
    "            current_weight_factor=hit_gene_weight_factor,\n",
    "            base_prefix=\"HIT_\"\n",
    "        )\n",
    "        current_full_features.update(hit_full_features)\n",
    "        \n",
    "        # Add core features of the HIT gene (unweighted for collapsing core comparison)\n",
    "        hit_core_features = extract_features_from_excel_gene_row(\n",
    "            gene_row_dict=hit_gene_row, \n",
    "            current_weight_factor=1, # Always 1 for core features, they are not redundantly weighted\n",
    "            base_prefix=\"HIT_CORE_\"\n",
    "        )\n",
    "        current_core_features.update(hit_core_features)\n",
    "\n",
    "        # Store initial label mapping for this neighborhood\n",
    "        full_neighborhood_labels_map[unique_neighborhood_label] = (organism_name, hit_gene_id, ssn_cluster_id, accession_id, None)\n",
    "\n",
    "        # Identify direct neighbors based on row index\n",
    "        hit_gene_idx_in_df = df_neighborhood.index[df_neighborhood[COL_GENE_ID] == hit_gene_id].tolist()[0]\n",
    "        \n",
    "        direct_neighbor_indices = []\n",
    "        if hit_gene_idx_in_df > 0: # Left neighbor\n",
    "            direct_neighbor_indices.append(hit_gene_idx_in_df - 1)\n",
    "        if hit_gene_idx_in_df < len(df_neighborhood) - 1: # Right neighbor\n",
    "            direct_neighbor_indices.append(hit_gene_idx_in_df + 1)\n",
    "\n",
    "        # Iterate through all genes in the neighborhood to extract features\n",
    "        for gene_idx, gene_row_series in df_neighborhood.iterrows():\n",
    "            gene_row_dict = gene_row_series.to_dict()\n",
    "            current_gene_id = gene_row_dict[COL_GENE_ID]\n",
    "            \n",
    "            # Skip the hit gene itself as its features are already processed with specific weights\n",
    "            if current_gene_id == hit_gene_id:\n",
    "                continue\n",
    "            \n",
    "            current_neighbor_weight_factor = 1\n",
    "            is_direct_neighbor = False\n",
    "\n",
    "            if gene_idx in direct_neighbor_indices:\n",
    "                current_neighbor_weight_factor = direct_neighbor_weight_factor\n",
    "                is_direct_neighbor = True\n",
    "            \n",
    "            # Extract full features for neighbors (weighted if direct)\n",
    "            neighbor_full_features = extract_features_from_excel_gene_row(\n",
    "                gene_row_dict=gene_row_dict, \n",
    "                current_weight_factor=current_neighbor_weight_factor,\n",
    "                base_prefix=\"N_\"\n",
    "            ) \n",
    "            current_full_features.update(neighbor_full_features)\n",
    "\n",
    "            # Extract core features for direct neighbors (unweighted for collapsing core comparison)\n",
    "            if is_direct_neighbor:\n",
    "                 neighbor_core_features = extract_features_from_excel_gene_row(\n",
    "                    gene_row_dict=gene_row_dict,\n",
    "                    current_weight_factor=1, # Always 1 for core features\n",
    "                    base_prefix=\"N_CORE_\"\n",
    "                )\n",
    "                 current_core_features.update(neighbor_core_features)\n",
    "\n",
    "        # Update the overall feature sets for this neighborhood\n",
    "        all_neighborhood_features[unique_neighborhood_label].update(current_full_features)\n",
    "        core_neighborhood_features[unique_neighborhood_label].update(current_core_features)\n",
    "    \n",
    "    \n",
    "    _write_and_print_internal(f\"Finished feature extraction in {time.time() - feature_extraction_start_time:.2f} seconds.\")\n",
    "    del excel_neighborhood_data # Free memory of raw Excel data\n",
    "    gc.collect() # Trigger garbage collection\n",
    "\n",
    "    if not all_neighborhood_features:\n",
    "        _write_and_print_internal(\"No gene neighborhoods with valid hit genes and features found. Exiting.\")\n",
    "        return {}, {}, {}\n",
    "\n",
    "    # Check for features before collapsing\n",
    "    all_unique_features_vocabulary_initial = sorted(list(set.union(*all_neighborhood_features.values())))\n",
    "    if not all_unique_features_vocabulary_initial:\n",
    "        _write_and_print_internal(\"No significant features extracted from any neighborhood. Cannot cluster.\")\n",
    "        # Return a dummy cluster for all neighborhoods to prevent errors down the line if the calling code expects it.\n",
    "        return {'All_Excel_Neighborhoods': {1: list(all_neighborhood_features.keys())}}, full_neighborhood_labels_map, {} \n",
    "\n",
    "    # Pre-clustering (collapsing) similar neighborhoods \n",
    "    if collapse_identical_neighborhoods:\n",
    "        collapsing_start_time = time.time()\n",
    "        _write_and_print_internal(f\"\\nPerforming collapsing of similar neighborhoods (Enabled).\")\n",
    "        final_neighborhood_features, final_neighborhood_labels_map, collapsed_groups_report = \\\n",
    "            _perform_collapsing(all_neighborhood_features, full_neighborhood_labels_map, \n",
    "                                core_neighborhood_features,\n",
    "                                collapse_core_similarity_threshold, collapse_full_neighborhood_similarity_threshold,\n",
    "                                output_prefix=\"  [Collapsing]\", \n",
    "                                report_file=report_file_handle, \n",
    "                                parallelize_pdist=parallelize_pdist)\n",
    "    else:\n",
    "        _write_and_print_internal(\"\\nCollapsing identical/similar neighborhoods is disabled. Proceeding with all original neighborhoods.\")\n",
    "        final_neighborhood_features = all_neighborhood_features\n",
    "        final_neighborhood_labels_map = full_neighborhood_labels_map\n",
    "        collapsed_groups_report = {}\n",
    "    \n",
    "    # Explicit memory cleanup for pre-collapse feature dictionaries\n",
    "    del all_neighborhood_features \n",
    "    del core_neighborhood_features \n",
    "    gc.collect() \n",
    "\n",
    "    # Re-check for features after collapsing\n",
    "    all_unique_features_vocabulary = sorted(list(set.union(*final_neighborhood_features.values())))\n",
    "    if not all_unique_features_vocabulary:\n",
    "        _write_and_print_internal(\"No significant features extracted from final neighborhoods after collapsing. Cannot cluster.\")\n",
    "        return {'All_Excel_Neighborhoods': {1: list(final_neighborhood_features.keys())}}, final_neighborhood_labels_map, collapsed_groups_report\n",
    "\n",
    "\n",
    "    # For Excel data, we cluster all neighborhoods together (no SSN differentiation by default)\n",
    "    # The structure of `clusters_output_dict` expects a key like ssn_id.\n",
    "    ssn_id_for_clustering = 'All_Excel_Neighborhoods' \n",
    "    neighborhood_labels_to_cluster = sorted(list(final_neighborhood_features.keys()))\n",
    "    \n",
    "    clusters_output_dict = defaultdict(dict) # Outer dict for SSN, inner for clusters within SSN\n",
    "\n",
    "    _write_and_print_internal(f\"\\n--- Processing all Excel neighborhoods ({len(neighborhood_labels_to_cluster)} neighborhoods) ---\")\n",
    "    plot_title_prefix = f\"Clustering by Gene Neighborhoods for {project_name}\"\n",
    "\n",
    "    num_neighborhoods_in_group = len(neighborhood_labels_to_cluster)\n",
    "    if num_neighborhoods_in_group < 2:\n",
    "        _write_and_print_internal(f\"  Skipping group '{ssn_id_for_clustering}': Not enough distinct neighborhoods ({num_neighborhoods_in_group}) for clustering. Requires at least 2.\")\n",
    "        # If less than 2, assign to a single cluster ID 1\n",
    "        clusters_output_dict[ssn_id_for_clustering] = {1: neighborhood_labels_to_cluster}\n",
    "    else:\n",
    "        # Prepare feature vectors for clustering\n",
    "        current_vocabulary = sorted(list(set.union(*[final_neighborhood_features[label] for label in neighborhood_labels_to_cluster])))\n",
    "        if not current_vocabulary:\n",
    "            _write_and_print_internal(f\"  No features found for group '{ssn_id_for_clustering}'. Cannot cluster.\")\n",
    "            clusters_output_dict[ssn_id_for_clustering] = {1: neighborhood_labels_to_cluster}\n",
    "        else:\n",
    "            feature_vector_creation_start = time.time()\n",
    "            \n",
    "            feature_to_idx = {feature: i for i, feature in enumerate(current_vocabulary)}\n",
    "            num_current_neighborhoods = len(neighborhood_labels_to_cluster)\n",
    "            num_current_features = len(current_vocabulary)\n",
    "\n",
    "            feature_vectors_lil = sp.lil_matrix((num_current_neighborhoods, num_current_features), dtype=np.int8)\n",
    "            for i, nh_id in enumerate(tqdm(neighborhood_labels_to_cluster, desc=f\"  Populating features for {ssn_id_for_clustering}\", leave=False)): \n",
    "                for feature in final_neighborhood_features[nh_id]:\n",
    "                    if feature in feature_to_idx:\n",
    "                        j = feature_to_idx[feature]\n",
    "                        feature_vectors_lil[i, j] = 1\n",
    "            feature_vectors_np = feature_vectors_lil.tocsr() # Convert to CSR for efficient row-wise access if needed\n",
    "\n",
    "            _write_and_print_internal(f\"  Feature vector creation for {num_neighborhoods_in_group} neighborhoods ({len(current_vocabulary)} features) took {time.time() - feature_vector_creation_start:.2f} seconds.\") \n",
    "            _write_and_print_internal(f\"  Matrix shape: {feature_vectors_np.shape}, NNZ: {feature_vectors_np.nnz}\") \n",
    "            gc.collect() \n",
    "\n",
    "            # Check for identical feature vectors (prevents issues with distance calculation/linkage)\n",
    "            if num_current_neighborhoods > 1 and all(\n",
    "                (feature_vectors_np[0] != feature_vectors_np[i]).nnz == 0 # Compare sparse rows\n",
    "                for i in range(1, num_current_neighborhoods)\n",
    "            ):\n",
    "                _write_and_print_internal(f\"  All neighborhoods in {plot_title_prefix} have identical features. No meaningful distance calculated. Skipping plotting.\")\n",
    "                clusters_output_dict[ssn_id_for_clustering] = {1: neighborhood_labels_to_cluster}\n",
    "            else:\n",
    "                plot_start = 0 \n",
    "                if plot_dendrogram: # Only start timer if plotting will occur\n",
    "                    plot_start = time.time() \n",
    "\n",
    "                distance_calc_start = time.time()\n",
    "                _write_and_print_internal(f\"  Calculating distances for {num_current_neighborhoods} neighborhoods...\") \n",
    "                distances = parallel_pdist_jaccard(feature_vectors_np, num_cores=-1 if parallelize_pdist else 1) \n",
    "                _write_and_print_internal(f\"  Distance calculation took {time.time() - distance_calc_start:.2f} seconds.\") \n",
    "\n",
    "                linkage_start = time.time()\n",
    "                _write_and_print_internal(f\"  Performing linkage for {num_current_neighborhoods} neighborhoods...\") \n",
    "                linked = linkage(distances, method='average')\n",
    "                _write_and_print_internal(f\"  Linkage calculation took {time.time() - linkage_start:.2f} seconds.\") \n",
    "                \n",
    "                # Explicit memory cleanup\n",
    "                del feature_vectors_np \n",
    "                del distances \n",
    "                gc.collect() \n",
    "                \n",
    "                if plot_dendrogram:\n",
    "                    # Organism Labels\n",
    "                    _write_and_print_internal(f\"  Generating dendrogram plots (Organism Labels)...\") \n",
    "                    _plot_dendrogram(linked, neighborhood_labels_to_cluster, final_neighborhood_labels_map, distance_threshold, \n",
    "                                    f\"{plot_title_prefix}\", 'organism', \n",
    "                                    original_input_sequence_id, # This is matched against accession_id, which is gene_id for Excel\n",
    "                                    save_plots, output_dir, output_formats, dpi, \n",
    "                                    min_plot_height, height_per_leaf, max_plot_height,\n",
    "                                    min_plot_width, width_per_leaf, max_plot_width,\n",
    "                                    report_file=report_file_handle)\n",
    "                    # ID Labels (using gene_id/accession_id)\n",
    "                    _write_and_print_internal(f\"  Generating dendrogram plots (ID Labels)...\") \n",
    "                    _plot_dendrogram(linked, neighborhood_labels_to_cluster, final_neighborhood_labels_map, distance_threshold, \n",
    "                                    f\"{plot_title_prefix}\", 'id', \n",
    "                                    original_input_sequence_id, \n",
    "                                    save_plots, output_dir, output_formats, dpi, \n",
    "                                    min_plot_height, height_per_leaf, max_plot_height,\n",
    "                                    min_plot_width, width_per_leaf, max_plot_width,\n",
    "                                    report_file=report_file_handle)\n",
    "                    _write_and_print_internal(f\"  Plotting took {time.time() - plot_start:.2f} seconds.\") \n",
    "                                \n",
    "                cluster_assignments = fcluster(linked, distance_threshold, criterion='distance')\n",
    "                current_clusters = defaultdict(list)\n",
    "                for i, cluster_id in enumerate(cluster_assignments):\n",
    "                    current_clusters[cluster_id].append(neighborhood_labels_to_cluster[i])\n",
    "                \n",
    "                clusters_output_dict[ssn_id_for_clustering] = current_clusters\n",
    "                _write_and_print_internal(f\"--- Finished processing Excel neighborhoods in {time.time() - start_time_overall:.2f} seconds. ---\") \n",
    "                del linked # Free memory\n",
    "                gc.collect() \n",
    "\n",
    "    _write_and_print_internal(f\"\\nTotal runtime: {time.time() - start_time_overall:.2f} seconds.\")\n",
    "    return clusters_output_dict, final_neighborhood_labels_map, collapsed_groups_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GNN Clustering Report (2025-09-11 22:17:45) ---\n",
      "Data Source: Excel File (PEP project_phosphonate gene clusters-20160423.xlsx)\n",
      "Jaccard Distance Threshold: 0.5\n",
      "Hit Gene Weight Factor: 10\n",
      "Direct Neighbor Weight Factor: 3\n",
      "Clustering all Excel neighborhoods together (SSN cluster differentiation not applicable for this data format).\n",
      "Collapsing identical/similar neighborhoods disabled.\n",
      "Distance calculation parallelism: Enabled (via joblib)\n",
      "SciPy/NumPy internal parallelism (OMP_NUM_THREADS): 12\n",
      "No specific original input sequence ID provided for highlighting.\n",
      "Plots saved to: gnn_cluster_plots_PEP in ['pdf'] formats at 300 DPI.\n",
      "Report also saved to: gnn_cluster_plots_PEP\\gnn_clustering_report_excel_input.txt\n",
      "----------------------------------------------------------------------\n",
      "Reading Excel file: PEP project_phosphonate gene clusters-20160423.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Excel sheets:   6%|▌         | 1/18 [00:00<00:02,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing sheet: NC5 LS110018 cluster38\n",
      "  Processing sheet: NC3 N272 cluster71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Excel sheets:  17%|█▋        | 3/18 [00:00<00:02,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing sheet: NC9 LS1801 cluster 63\n",
      "  Processing sheet: 182 LS2039-cluster 17\n",
      "  Warning: Valid header row not found in sheet '182 LS2039-cluster 17'. Skipping sheet.\n",
      "  Processing sheet: NC31-LS130724-scaf1-no cluster\n",
      "  Processing sheet: 185-LS784-cluster 50\n",
      "  Processing sheet: 131-LS477-cluster 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Excel sheets:  61%|██████    | 11/18 [00:00<00:00, 21.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing sheet: NC10 LS130053\n",
      "  Processing sheet: NC24 LS132251\n",
      "  Processing sheet: NC31 LS130724\n",
      "  Processing sheet: NC4-LS2542-cluster100\n",
      "  Processing sheet: 71-LS130084-cluster 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Excel sheets: 100%|██████████| 18/18 [00:01<00:00, 17.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing sheet: 71-LS130084-cluster 128\n",
      "  Processing sheet: 78-LS131321-cluster 37\n",
      "  Processing sheet: 170-LS795-cluster 72\n",
      "  Processing sheet: 177-LS120054-no cluster-7\n",
      "  Processing sheet: NC30-LS131440-sca33-no cluster\n",
      "  Processing sheet: Sheet1\n",
      "  Warning: Valid header row not found in sheet 'Sheet1'. Skipping sheet.\n",
      "Successfully read data from 16 sheets.\n",
      "Processing features for 16 gene neighborhoods from Excel sheets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features from Excel neighborhoods: 100%|██████████| 16/16 [00:00<00:00, 54.86neighborhood/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction in 0.30 seconds.\n",
      "\n",
      "Collapsing identical/similar neighborhoods is disabled. Proceeding with all original neighborhoods.\n",
      "\n",
      "--- Processing all Excel neighborhoods (16 neighborhoods) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature vector creation for 16 neighborhoods (502 features) took 0.03 seconds.\n",
      "  Matrix shape: (16, 502), NNZ: 903\n",
      "  Calculating distances for 16 neighborhoods...\n",
      "  Pre-calculating 16 feature sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature set pre-calculation took 0.01 seconds.\n",
      "  Running Jaccard distance sequentially for N=16 (below parallel threshold or num_cores=1).\n",
      "  Distance calculation took 0.26 seconds.\n",
      "  Performing linkage for 16 neighborhoods...\n",
      "  Linkage calculation took 0.00 seconds.\n",
      "  Generating dendrogram plots (Organism Labels)...\n",
      "  Generating dendrogram plots (ID Labels)...\n",
      "  Plotting took 2.89 seconds.\n",
      "--- Finished processing Excel neighborhoods in 12.33 seconds. ---\n",
      "\n",
      "Total runtime: 12.66 seconds.\n",
      "\n",
      "--- Final Clustering Results ---\n",
      "\n",
      "### Results for Group: All_Excel_Neighborhoods ###\n",
      "  Cluster 1: 1 neighborhoods\n",
      "    - Organism: NC24 LS132251, Hit Gene ID: ctg4_81 (Internal NH ID: NC24 LS132251_ctg4_81)\n",
      "  Cluster 2: 1 neighborhoods\n",
      "    - Organism: NC9 LS1801 cluster 63, Hit Gene ID: ctg8_386 (Internal NH ID: NC9 LS1801 cluster 63_ctg8_386)\n",
      "  Cluster 3: 1 neighborhoods\n",
      "    - Organism: NC10 LS130053, Hit Gene ID: ctg4_452 (Internal NH ID: NC10 LS130053_ctg4_452)\n",
      "  Cluster 4: 1 neighborhoods\n",
      "    - Organism: NC3 N272 cluster71, Hit Gene ID: ctg12_117 (Internal NH ID: NC3 N272 cluster71_ctg12_117)\n",
      "  Cluster 5: 3 neighborhoods\n",
      "    - Organism: 170-LS795-cluster 72, Hit Gene ID: ctg9_112 (Internal NH ID: 170-LS795-cluster 72_ctg9_112)\n",
      "    - Organism: 185-LS784-cluster 50, Hit Gene ID: ctg5-93 (Internal NH ID: 185-LS784-cluster 50_ctg5-93)\n",
      "    - Organism: NC5 LS110018 cluster38, Hit Gene ID: ctg5_209 (Internal NH ID: NC5 LS110018 cluster38_ctg5_209)\n",
      "  Cluster 6: 1 neighborhoods\n",
      "    - Organism: 78-LS131321-cluster 37, Hit Gene ID: ctg5_253 (Internal NH ID: 78-LS131321-cluster 37_ctg5_253)\n",
      "  Cluster 7: 1 neighborhoods\n",
      "    - Organism: 71-LS130084-cluster 114, Hit Gene ID: ctg27_85 (Internal NH ID: 71-LS130084-cluster 114_ctg27_85)\n",
      "  Cluster 8: 1 neighborhoods\n",
      "    - Organism: NC30-LS131440-sca33-no cluster, Hit Gene ID: NC30GM006768 (Internal NH ID: NC30-LS131440-sca33-no cluster_NC30GM006768)\n",
      "  Cluster 9: 1 neighborhoods\n",
      "    - Organism: 71-LS130084-cluster 128, Hit Gene ID: ctg39_5 (Internal NH ID: 71-LS130084-cluster 128_ctg39_5)\n",
      "  Cluster 10: 1 neighborhoods\n",
      "    - Organism: NC31-LS130724-scaf1-no cluster, Hit Gene ID: NC31GM000149 (Internal NH ID: NC31-LS130724-scaf1-no cluster_NC31GM000149)\n",
      "  Cluster 11: 1 neighborhoods\n",
      "    - Organism: 177-LS120054-no cluster-7, Hit Gene ID: 177GM005631 (Internal NH ID: 177-LS120054-no cluster-7_177GM005631)\n",
      "  Cluster 12: 1 neighborhoods\n",
      "    - Organism: NC31 LS130724, Hit Gene ID: NC31GM002870 (Internal NH ID: NC31 LS130724_NC31GM002870)\n",
      "  Cluster 13: 1 neighborhoods\n",
      "    - Organism: NC4-LS2542-cluster100, Hit Gene ID: ctg18_40 (Internal NH ID: NC4-LS2542-cluster100_ctg18_40)\n",
      "  Cluster 14: 1 neighborhoods\n",
      "    - Organism: 131-LS477-cluster 26, Hit Gene ID: ctg_477 (Internal NH ID: 131-LS477-cluster 26_ctg_477)\n",
      "  ------------------------------\n",
      "\n",
      "--- Report End ---\n"
     ]
    }
   ],
   "source": [
    "EXCEL_FILE_PATH = 'PEP project_phosphonate gene clusters-20160423.xlsx'\n",
    "project_name = 'PEP gene clusters'\n",
    "\n",
    "ORIGINAL_INPUT_SEQUENCE_ID = None \n",
    "\n",
    "# Column names in the Excel sheets\n",
    "EXCEL_COL_LENGTH = '长度(aa)'\n",
    "EXCEL_COL_PFAM = 'pfam'\n",
    "EXCEL_COL_HIT_GENE = 'hit_gene'\n",
    "EXCEL_HIT_GENE_MARKER = 'yes' # Value indicating the hit gene\n",
    "\n",
    "# You can change this setting here or keep the global config\n",
    "chosen_distance_threshold = 0.5         # Change as needed\n",
    "PARALLELIZE_PDIST_ENABLED = True        # Set to True to enable parallel pdist.\n",
    "\n",
    "# Configuration for collapsing similar neighborhoods (Local Override)\n",
    "COLLAPSE_IDENTICAL_NEIGHBORHOODS_ACTIVE = False \n",
    "COLLAPSE_CORE_SIMILARITY_THRESHOLD_ACTIVE = 0.0 # Use 0.0 for exact match of hit+direct neighbors\n",
    "COLLAPSE_FULL_NEIGHBORHOOD_SIMILARITY_THRESHOLD_ACTIVE = 0.3 # e.g., 0.3 for 70% similarity of full neighborhood\n",
    "\n",
    "# Prepare report file\n",
    "report_suffix = \"_excel_input\" # Changed suffix for Excel input\n",
    "report_filename = f\"{REPORT_FILENAME_BASE}{report_suffix}.txt\"\n",
    "report_path = os.path.join(OUTPUT_DIR, report_filename)\n",
    "    \n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR) # Ensure output directory exists for report and plots\n",
    "\n",
    "with open(report_path, 'w') as report_file:\n",
    "    # Redefine write_and_print to use the local report_file handle\n",
    "    def write_and_print_to_file(text):\n",
    "        print(text)\n",
    "        report_file.write(text + '\\n')\n",
    "\n",
    "    # Use write_and_print_to_file for all report output\n",
    "    write_and_print_to_file(f\"\\n--- GNN Clustering Report ({datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}) ---\") \n",
    "    write_and_print_to_file(f\"Data Source: Excel File ({EXCEL_FILE_PATH})\") \n",
    "    write_and_print_to_file(f\"Jaccard Distance Threshold: {chosen_distance_threshold}\")\n",
    "    write_and_print_to_file(f\"Hit Gene Weight Factor: {HIT_GENE_WEIGHT_FACTOR}\")\n",
    "    write_and_print_to_file(f\"Direct Neighbor Weight Factor: {DIRECT_NEIGHBOR_WEIGHT_FACTOR}\")\n",
    "    \n",
    "    # Report that SSN differentiation is not applicable for Excel\n",
    "    write_and_print_to_file(\"Clustering all Excel neighborhoods together (SSN cluster differentiation not applicable for this data format).\")\n",
    "    \n",
    "    if COLLAPSE_IDENTICAL_NEIGHBORHOODS_ACTIVE:\n",
    "        write_and_print_to_file(f\"Collapsing identical/similar neighborhoods enabled:\")\n",
    "        write_and_print_to_file(f\"  Stage 1 (Hit+Direct Neighbor Core): Threshold {COLLAPSE_CORE_SIMILARITY_THRESHOLD_ACTIVE}\")\n",
    "        write_and_print_to_file(f\"  Stage 2 (Full Neighborhood): Threshold {COLLAPSE_FULL_NEIGHBORHOOD_SIMILARITY_THRESHOLD_ACTIVE}\")\n",
    "    else:\n",
    "        write_and_print_to_file(\"Collapsing identical/similar neighborhoods disabled.\")\n",
    "\n",
    "    write_and_print_to_file(f\"Distance calculation parallelism: {'Enabled (via joblib)' if PARALLELIZE_PDIST_ENABLED else 'Disabled (sequential custom Jaccard)'}\")\n",
    "    write_and_print_to_file(f\"SciPy/NumPy internal parallelism (OMP_NUM_THREADS): {os.environ.get('OMP_NUM_THREADS', 'Not set (defaults will apply)')}\")\n",
    "    \n",
    "    if ORIGINAL_INPUT_SEQUENCE_ID:\n",
    "        write_and_print_to_file(f\"Original Input Sequence Gene ID for highlighting: '{ORIGINAL_INPUT_SEQUENCE_ID}' (colored '{HIGHLIGHT_COLOR}')\")\n",
    "    else:\n",
    "        write_and_print_to_file(\"No specific original input sequence ID provided for highlighting.\")\n",
    "    write_and_print_to_file(f\"Plots saved to: {OUTPUT_DIR} in {OUTPUT_FORMATS} formats at {DPI} DPI.\")\n",
    "    write_and_print_to_file(f\"Report also saved to: {report_path}\")\n",
    "    write_and_print_to_file(\"-\" * 70)\n",
    "\n",
    "    clusters_by_ssn, final_labels_map, collapsed_groups_report = cluster_gene_neighborhoods_from_excel(\n",
    "                                            excel_path=EXCEL_FILE_PATH,\n",
    "                                            project_name=project_name,\n",
    "                                            hit_gene_weight_factor=HIT_GENE_WEIGHT_FACTOR,\n",
    "                                            direct_neighbor_weight_factor=DIRECT_NEIGHBOR_WEIGHT_FACTOR,\n",
    "                                            original_input_sequence_id=ORIGINAL_INPUT_SEQUENCE_ID,\n",
    "                                            distance_threshold=chosen_distance_threshold,\n",
    "                                            plot_dendrogram=True,\n",
    "                                            save_plots=SAVE_PLOTS,\n",
    "                                            output_dir=OUTPUT_DIR,\n",
    "                                            output_formats=OUTPUT_FORMATS, \n",
    "                                            dpi=DPI,\n",
    "                                            min_plot_height=MIN_PLOT_HEIGHT,\n",
    "                                            height_per_leaf=HEIGHT_PER_LEAF, \n",
    "                                            max_plot_height=MAX_PLOT_HEIGHT,\n",
    "                                            min_plot_width=MIN_PLOT_WIDTH, \n",
    "                                            width_per_leaf=WIDTH_PER_LEAF, \n",
    "                                            max_plot_width=MAX_PLOT_WIDTH,\n",
    "                                            report_file_handle=report_file,\n",
    "                                            parallelize_pdist=PARALLELIZE_PDIST_ENABLED,\n",
    "                                            collapse_identical_neighborhoods=COLLAPSE_IDENTICAL_NEIGHBORHOODS_ACTIVE,\n",
    "                                            collapse_core_similarity_threshold=COLLAPSE_CORE_SIMILARITY_THRESHOLD_ACTIVE,\n",
    "                                            collapse_full_neighborhood_similarity_threshold=COLLAPSE_FULL_NEIGHBORHOOD_SIMILARITY_THRESHOLD_ACTIVE\n",
    "                                        )\n",
    "\n",
    "\n",
    "    if clusters_by_ssn:\n",
    "        write_and_print_to_file(\"\\n--- Final Clustering Results ---\")\n",
    "        # For Excel input, there will typically be only one \"SSN group\" called 'All_Excel_Neighborhoods'\n",
    "        for ssn_id, clusters_in_ssn in sorted(clusters_by_ssn.items(), key=lambda item: str(item[0])):\n",
    "            write_and_print_to_file(f\"\\n### Results for Group: {ssn_id} ###\")\n",
    "            if not clusters_in_ssn:\n",
    "                write_and_print_to_file(\"  No clusters formed for this group, or insufficient data.\")\n",
    "                continue\n",
    "\n",
    "            for cluster_id, neighborhoods_in_cluster in sorted(clusters_in_ssn.items()):\n",
    "                write_and_print_to_file(f\"  Cluster {cluster_id}: {len(neighborhoods_in_cluster)} neighborhoods\")\n",
    "                for nh_id in neighborhoods_in_cluster:\n",
    "                    organism_name, hit_id_internal, _, accession_id, collapsed_info = final_labels_map.get(nh_id, ('UNKNOWN', 'UNKNOWN', None, 'UNKNOWN', None))\n",
    "                    \n",
    "                    # Highlight based on accession_id (which is gene_id for Excel)\n",
    "                    highlight_indicator = \" (ORIGINAL INPUT)\" if accession_id == ORIGINAL_INPUT_SEQUENCE_ID else \"\"\n",
    "                    collapsed_suffix = \"\"\n",
    "                    if collapsed_info:\n",
    "                        count, letter_code = collapsed_info\n",
    "                        collapsed_suffix = f\" (Collapsed: {count} neighborhoods, Ref: {letter_code})\"\n",
    "                    \n",
    "                    write_and_print_to_file(f\"    - Organism: {organism_name}, Hit Gene ID: {accession_id}{highlight_indicator}{collapsed_suffix} (Internal NH ID: {nh_id})\")\n",
    "            write_and_print_to_file(\"  \" + \"-\" * 30)\n",
    "        \n",
    "        if collapsed_groups_report:\n",
    "            write_and_print_to_file(\"\\n--- Detailed Report on Collapsed Neighborhood Groups ---\")\n",
    "            for code, group_data in sorted(collapsed_groups_report.items()):\n",
    "                write_and_print_to_file(f\"  Group ({code}): Representative: {group_data['representative']} (Total: {group_data['count']} members)\")\n",
    "                for member_nh_id in group_data['members']:\n",
    "                    member_organism, member_hit_id, _, member_accession, _ = final_labels_map.get(member_nh_id, ('UNKNOWN', 'UNKNOWN', None, 'UNKNOWN', None))\n",
    "                    write_and_print_to_file(f\"    - {member_organism} (Gene ID: {member_accession}) (Internal Hit ID: {member_hit_id}) (NH ID: {member_nh_id})\")\n",
    "            write_and_print_to_file(\"-------------------------------------------------------\")\n",
    "    else:\n",
    "        write_and_print_to_file(\"\\nNo clusters formed at all. This could mean your Excel file is empty, or no features were extracted after filtering, or not enough valid neighborhoods for clustering were found.\")\n",
    "\n",
    "    write_and_print_to_file(\"\\n--- Report End ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
