{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7c6f5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import gc\n",
    "import re\n",
    "import sqlite3\n",
    "import string\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.spatial import ConvexHull\n",
    "from threadpoolctl import threadpool_limits\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Parallel / BLAS configuration\n",
    "# ----------------------------------------------------------------------\n",
    "num_logical_cores = os.cpu_count()\n",
    "if num_logical_cores:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_logical_cores)\n",
    "else:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ----------------------------------------------------------------------\n",
    "GENES_TABLE = \"attributes\"\n",
    "NEIGHBORS_TABLE = \"neighbors\"\n",
    "\n",
    "COL_NEIGHBORHOOD_ID = \"organism\"\n",
    "COL_GENE_ID = \"id\"\n",
    "COL_LINKING_KEY = \"id\"\n",
    "COL_ACCESSION_ID = \"accession\"\n",
    "COL_FUNCTION_DESC = \"desc\"\n",
    "COL_PFAM_IDS = \"family\"\n",
    "COL_INTERPRO_IDS = \"ipro_family\"\n",
    "COL_REL_START = \"rel_start\"\n",
    "COL_REL_STOP = \"rel_stop\"\n",
    "COL_SSN_CLUSTER_ID = \"cluster_num\"\n",
    "\n",
    "HIT_GENE_WEIGHT_FACTOR = 10\n",
    "DIRECT_NEIGHBOR_WEIGHT_FACTOR = 3\n",
    "\n",
    "DEFAULT_SSN_CLUSTER_VALUE_TO_FILTER = [None, 0]\n",
    "\n",
    "SAVE_PLOTS = True\n",
    "OUTPUT_DIR = \"gnn_cluster_plots_circular\"\n",
    "REPORT_FILENAME_BASE = \"gnn_clustering_report\"\n",
    "OUTPUT_FORMATS = [\"pdf\"]\n",
    "DPI = 600\n",
    "HIGHLIGHT_COLOR = \"red\"\n",
    "\n",
    "COLLAPSE_IDENTICAL_NEIGHBORHOODS = True\n",
    "COLLAPSE_CORE_SIMILARITY_THRESHOLD = 0.0\n",
    "COLLAPSE_FULL_NEIGHBORHOOD_SIMILARITY_THRESHOLD = 0.3\n",
    "\n",
    "MIN_ITEMS_FOR_PARALLEL_PROCESSING = 20\n",
    "\n",
    "# Precompiled regex / constants\n",
    "_IPR_REGEX = re.compile(r\"IPR\\d+\", re.IGNORECASE)\n",
    "_PFAM_REGEX = re.compile(r\"PF\\d+\", re.IGNORECASE)\n",
    "_UNINFORMATIVE_TERMS = frozenset([\"none\", \"\", \"null\", \"uncharacterized protein\"])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Basic feature parsing\n",
    "# ----------------------------------------------------------------------\n",
    "def parse_annotation_string(annotation_str, prefix=\"\"):\n",
    "    if not isinstance(annotation_str, str) or pd.isna(annotation_str):\n",
    "        return set()\n",
    "    if annotation_str.lower().strip() in _UNINFORMATIVE_TERMS:\n",
    "        return set()\n",
    "\n",
    "    features = set()\n",
    "    parts = [p.strip() for p in re.split(r\"[-;]\", annotation_str) if p.strip()]\n",
    "\n",
    "    for part in parts:\n",
    "        if part.lower().strip() in _UNINFORMATIVE_TERMS:\n",
    "            continue\n",
    "\n",
    "        if _IPR_REGEX.match(part):\n",
    "            features.add(f\"{prefix}{part.upper()}\")\n",
    "        elif _PFAM_REGEX.match(part):\n",
    "            features.add(f\"{prefix}{part.upper()}\")\n",
    "        else:\n",
    "            clean_part = re.sub(r\"\\s+\", \" \", part).lower().strip()\n",
    "            if clean_part:\n",
    "                features.add(f\"{prefix}{clean_part}\")\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_features_from_gene_row(\n",
    "    gene_row,\n",
    "    current_weight_factor=1,\n",
    "    base_prefix=\"N_\",\n",
    "    include_desc=True,\n",
    "    include_pfam=True,\n",
    "    include_interpro=True,\n",
    "):\n",
    "    raw_features = set()\n",
    "\n",
    "    if include_desc:\n",
    "        raw_features.update(parse_annotation_string(gene_row[COL_FUNCTION_DESC]))\n",
    "    if include_pfam:\n",
    "        raw_features.update(parse_annotation_string(gene_row[COL_PFAM_IDS]))\n",
    "    if include_interpro:\n",
    "        raw_features.update(parse_annotation_string(gene_row[COL_INTERPRO_IDS]))\n",
    "\n",
    "    if current_weight_factor <= 1:\n",
    "        return {f\"{base_prefix}{f}\" for f in raw_features}\n",
    "\n",
    "    features = set()\n",
    "    for f in raw_features:\n",
    "        for i in range(current_weight_factor):\n",
    "            features.add(f\"{base_prefix}{f}_w{i}\")\n",
    "    return features\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Distance calculation (Jaccard) with optional parallelism\n",
    "# ----------------------------------------------------------------------\n",
    "def parallel_pdist_jaccard(feature_matrix, num_cores=-1):\n",
    "    \"\"\"\n",
    "    Compute condensed Jaccard distance for a binary sparse matrix (CSR).\n",
    "    \"\"\"\n",
    "    if not isinstance(feature_matrix, sp.csr_matrix):\n",
    "        if isinstance(feature_matrix, (sp.csc_matrix, sp.lil_matrix, sp.coo_matrix)):\n",
    "            feature_matrix = feature_matrix.tocsr()\n",
    "        else:\n",
    "            raise TypeError(\"feature_matrix must be a SciPy sparse matrix\")\n",
    "\n",
    "    n_samples = feature_matrix.shape[0]\n",
    "    if n_samples <= 1:\n",
    "        return np.array([])\n",
    "\n",
    "    if num_cores == -1:\n",
    "        detected = os.cpu_count()\n",
    "        num_cores = detected if detected and detected > 0 else 1\n",
    "    elif num_cores == 0:\n",
    "        num_cores = 1\n",
    "\n",
    "    # Precompute row index sets once\n",
    "    print(f\"  Pre-computing feature sets for N={n_samples} ...\")\n",
    "    t0 = time.time()\n",
    "    feature_sets = [\n",
    "        set(feature_matrix.indices[feature_matrix.indptr[i] : feature_matrix.indptr[i + 1]])\n",
    "        for i in tqdm(range(n_samples), desc=\"  Feature sets\", leave=False)\n",
    "    ]\n",
    "    print(f\"  Done in {time.time() - t0:.2f}s\")\n",
    "    del feature_matrix\n",
    "    gc.collect()\n",
    "\n",
    "    def _dist_chunk(start_i, end_i, sets_ref, n_total):\n",
    "        with threadpool_limits(limits=1, user_api=\"blas\"):\n",
    "            chunk = []\n",
    "            for i in range(start_i, end_i):\n",
    "                si = sets_ref[i]\n",
    "                for j in range(i + 1, n_total):\n",
    "                    sj = sets_ref[j]\n",
    "                    inter = len(si & sj)\n",
    "                    union = len(si | sj)\n",
    "                    d = 0.0 if union == 0 else 1.0 - inter / union\n",
    "                    chunk.append(d)\n",
    "            return chunk\n",
    "\n",
    "    total_i = n_samples - 1\n",
    "    if total_i <= 0:\n",
    "        return np.array([])\n",
    "\n",
    "    if n_samples < MIN_ITEMS_FOR_PARALLEL_PROCESSING or num_cores == 1:\n",
    "        print(f\"  Using sequential Jaccard (N={n_samples})\")\n",
    "        res = [_dist_chunk(0, total_i, feature_sets, n_samples)]\n",
    "    else:\n",
    "        print(f\"  Using parallel Jaccard (N={n_samples}, cores={num_cores})\")\n",
    "        num_tasks = min(total_i, num_cores * 4)\n",
    "        chunk_size = max(1, (total_i + num_tasks - 1) // num_tasks)\n",
    "        ranges = [(k, min(k + chunk_size, total_i)) for k in range(0, total_i, chunk_size)]\n",
    "\n",
    "        tasks = [\n",
    "            delayed(_dist_chunk)(start, end, feature_sets, n_samples) for start, end in ranges\n",
    "        ]\n",
    "        res = Parallel(n_jobs=num_cores, backend=\"loky\", verbose=0)(tasks)\n",
    "\n",
    "    return np.concatenate(res)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Collapsing similar neighborhoods\n",
    "# ----------------------------------------------------------------------\n",
    "def _perform_collapsing(\n",
    "    all_neighborhood_features,\n",
    "    full_neighborhood_labels_map,\n",
    "    core_neighborhood_features,\n",
    "    collapse_core_similarity_threshold,\n",
    "    collapse_full_neighborhood_similarity_threshold,\n",
    "    output_prefix=\"\",\n",
    "    report_file=None,\n",
    "    parallelize_pdist=False,\n",
    "):\n",
    "\n",
    "    def log(msg):\n",
    "        print(msg)\n",
    "        if report_file:\n",
    "            report_file.write(msg + \"\\n\")\n",
    "\n",
    "    log(\n",
    "        f\"{output_prefix}  Collapsing neighborhoods (core thr: {collapse_core_similarity_threshold}, \"\n",
    "        f\"full thr: {collapse_full_neighborhood_similarity_threshold})\"\n",
    "    )\n",
    "    t0_all = time.time()\n",
    "    collapsed_groups_report = {}\n",
    "\n",
    "    # ---- Stage 1: core-based grouping ----\n",
    "    labels = sorted(core_neighborhood_features.keys())\n",
    "    if len(labels) < 2:\n",
    "        log(f\"{output_prefix}  <2 neighborhoods; skipping collapsing.\")\n",
    "        return all_neighborhood_features, full_neighborhood_labels_map, collapsed_groups_report\n",
    "\n",
    "    core_vocab = sorted(set.union(*core_neighborhood_features.values()))\n",
    "    if not core_vocab:\n",
    "        log(f\"{output_prefix}  No core features; skipping collapsing.\")\n",
    "        return all_neighborhood_features, full_neighborhood_labels_map, collapsed_groups_report\n",
    "\n",
    "    log(f\"{output_prefix}  Stage1: building core sparse matrix\")\n",
    "    feat_to_idx = {f: i for i, f in enumerate(core_vocab)}\n",
    "    n_nh = len(labels)\n",
    "    n_feat = len(core_vocab)\n",
    "    mat_lil = sp.lil_matrix((n_nh, n_feat), dtype=np.int8)\n",
    "    for i, nh in enumerate(tqdm(labels, desc=f\"{output_prefix}  Core features\", leave=False)):\n",
    "        for f in core_neighborhood_features[nh]:\n",
    "            j = feat_to_idx[f]\n",
    "            mat_lil[i, j] = 1\n",
    "    core_mat = mat_lil.tocsr()\n",
    "    del mat_lil\n",
    "    gc.collect()\n",
    "\n",
    "    if core_mat.shape[0] < 2:\n",
    "        log(f\"{output_prefix}  <2 core vectors; skipping collapsing.\")\n",
    "        return all_neighborhood_features, full_neighborhood_labels_map, collapsed_groups_report\n",
    "\n",
    "    # Check if all identical\n",
    "    if n_nh > 1 and all((core_mat[0] != core_mat[i]).nnz == 0 for i in range(1, n_nh)):\n",
    "        # everything identical -> one group\n",
    "        pre_clusters = {labels[i]: 1 for i in range(len(labels))}\n",
    "        log(f\"{output_prefix}  All core vectors identical; treating as one initial group.\")\n",
    "    else:\n",
    "        log(f\"{output_prefix}  Stage1: computing core Jaccard distances\")\n",
    "        t0 = time.time()\n",
    "        dists = parallel_pdist_jaccard(core_mat, num_cores=-1 if parallelize_pdist else 1)\n",
    "        log(f\"{output_prefix}  Stage1: distance calc in {time.time() - t0:.2f}s\")\n",
    "        from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "        link_core = linkage(dists, method=\"average\")\n",
    "        pre_clusters_array = fcluster(link_core, collapse_core_similarity_threshold, criterion=\"distance\")\n",
    "        pre_clusters = {labels[i]: pre_clusters_array[i] for i in range(len(labels))}\n",
    "\n",
    "    initial_core_groups = defaultdict(list)\n",
    "    for nh, cid in pre_clusters.items():\n",
    "        initial_core_groups[cid].append(nh)\n",
    "\n",
    "    log(\n",
    "        f\"{output_prefix}  Stage1: {len(initial_core_groups)} core groups \"\n",
    "        f\"({time.time() - t0_all:.2f}s partial)\"\n",
    "    )\n",
    "\n",
    "    del core_mat\n",
    "    if \"dists\" in locals():\n",
    "        del dists\n",
    "    gc.collect()\n",
    "\n",
    "    # ---- Stage 2: full neighborhood collapsing within each core group ----\n",
    "    def gen_letter(idx):\n",
    "        if idx < 26:\n",
    "            return string.ascii_uppercase[idx]\n",
    "        first = (idx // 26) - 1\n",
    "        second = idx % 26\n",
    "        return f\"{string.ascii_uppercase[first]}{string.ascii_uppercase[second]}\"\n",
    "\n",
    "    log(f\"{output_prefix}  Stage2: full-neighborhood collapsing\")\n",
    "\n",
    "    def process_core_group_chunk(\n",
    "        group_ids,\n",
    "        all_feat_ref,\n",
    "        labels_map_ref,\n",
    "        thr_full,\n",
    "        allow_parallel,\n",
    "    ):\n",
    "        out = []\n",
    "        collapsed_count = 0\n",
    "        cores_inner = -1 if allow_parallel else 1\n",
    "\n",
    "        with threadpool_limits(limits=1, user_api=\"blas\"):\n",
    "            for gid in group_ids:\n",
    "                members = initial_core_groups[gid]\n",
    "                if len(members) < 2:\n",
    "                    m = members[0]\n",
    "                    out.append((m, all_feat_ref[m], labels_map_ref[m], None))\n",
    "                    continue\n",
    "\n",
    "                vocab = sorted(set.union(*[all_feat_ref[m] for m in members]))\n",
    "                if not vocab:\n",
    "                    assignments = {m: 1 for m in members}\n",
    "                else:\n",
    "                    ft_idx = {f: i for i, f in enumerate(vocab)}\n",
    "                    n_m = len(members)\n",
    "                    m_lil = sp.lil_matrix((n_m, len(vocab)), dtype=np.int8)\n",
    "                    for i, nh in enumerate(members):\n",
    "                        for f in all_feat_ref[nh]:\n",
    "                            j = ft_idx[f]\n",
    "                            m_lil[i, j] = 1\n",
    "                    mat = m_lil.tocsr()\n",
    "\n",
    "                    if n_m > 1 and all((mat[0] != mat[i]).nnz == 0 for i in range(1, n_m)):\n",
    "                        assignments = {m: 1 for m in members}\n",
    "                    else:\n",
    "                        d = parallel_pdist_jaccard(mat, num_cores=cores_inner)\n",
    "                        if d.size == 0:\n",
    "                            assignments = {m: 1 for m in members}\n",
    "                        else:\n",
    "                            link = linkage(d, method=\"average\")\n",
    "                            arr = fcluster(link, thr_full, criterion=\"distance\")\n",
    "                            assignments = {members[i]: arr[i] for i in range(len(members))}\n",
    "                    del mat\n",
    "                    if \"d\" in locals():\n",
    "                        del d\n",
    "                    if \"link\" in locals():\n",
    "                        del link\n",
    "                    gc.collect()\n",
    "\n",
    "                groups = defaultdict(list)\n",
    "                for nh, cid in assignments.items():\n",
    "                    groups[cid].append(nh)\n",
    "\n",
    "                for cid, collapsed_members in sorted(groups.items()):\n",
    "                    if len(collapsed_members) > 1:\n",
    "                        collapsed_count += len(collapsed_members) - 1\n",
    "                        rep = collapsed_members[0]\n",
    "                        union_features = set()\n",
    "                        for nh in collapsed_members:\n",
    "                            union_features.update(all_feat_ref[nh])\n",
    "                        out.append((rep, union_features, labels_map_ref[rep], collapsed_members))\n",
    "                    else:\n",
    "                        m = collapsed_members[0]\n",
    "                        out.append((m, all_feat_ref[m], labels_map_ref[m], None))\n",
    "\n",
    "        return out, collapsed_count\n",
    "\n",
    "    all_ids = sorted(initial_core_groups.keys())\n",
    "    n_core_groups = len(all_ids)\n",
    "    if n_core_groups < MIN_ITEMS_FOR_PARALLEL_PROCESSING or not parallelize_pdist:\n",
    "        chunk_results = [\n",
    "            process_core_group_chunk(\n",
    "                all_ids,\n",
    "                all_neighborhood_features,\n",
    "                full_neighborhood_labels_map,\n",
    "                collapse_full_neighborhood_similarity_threshold,\n",
    "                False,\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        n_cores = os.cpu_count() or 1\n",
    "        chunk_size = max(1, n_core_groups // n_cores)\n",
    "        chunks = [all_ids[i : i + chunk_size] for i in range(0, n_core_groups, chunk_size)]\n",
    "        chunk_results = Parallel(n_jobs=n_cores, backend=\"loky\", verbose=0)(\n",
    "            delayed(process_core_group_chunk)(\n",
    "                ch,\n",
    "                all_neighborhood_features,\n",
    "                full_neighborhood_labels_map,\n",
    "                collapse_full_neighborhood_similarity_threshold,\n",
    "                parallelize_pdist,\n",
    "            )\n",
    "            for ch in chunks\n",
    "        )\n",
    "\n",
    "    final_neighborhood_features = {}\n",
    "    final_neighborhood_labels_map = {}\n",
    "    collapsed_total = 0\n",
    "    letter_counter = 0\n",
    "\n",
    "    for res, local_count in chunk_results:\n",
    "        collapsed_total += local_count\n",
    "        for rep_label, feats, label_entry, collapsed_members in res:\n",
    "            if collapsed_members is not None:\n",
    "                code = gen_letter(letter_counter)\n",
    "                letter_counter += 1\n",
    "                org, hit_id, ssn_id, acc, _ = label_entry\n",
    "                final_neighborhood_labels_map[rep_label] = (\n",
    "                    org,\n",
    "                    hit_id,\n",
    "                    ssn_id,\n",
    "                    acc,\n",
    "                    (len(collapsed_members), code),\n",
    "                )\n",
    "                collapsed_groups_report[code] = {\n",
    "                    \"representative\": rep_label,\n",
    "                    \"members\": sorted(collapsed_members),\n",
    "                    \"count\": len(collapsed_members),\n",
    "                }\n",
    "                final_neighborhood_features[rep_label] = feats\n",
    "            else:\n",
    "                final_neighborhood_features[rep_label] = feats\n",
    "                final_neighborhood_labels_map[rep_label] = label_entry\n",
    "\n",
    "    if collapsed_total > 0:\n",
    "        log(\n",
    "            f\"{output_prefix}  Collapsed {collapsed_total} neighborhoods -> \"\n",
    "            f\"{len(final_neighborhood_features)} unique entries \"\n",
    "            f\"({time.time() - t0_all:.2f}s total).\"\n",
    "        )\n",
    "    else:\n",
    "        log(f\"{output_prefix}  No collapsing performed ({time.time() - t0_all:.2f}s).\")\n",
    "\n",
    "    return final_neighborhood_features, final_neighborhood_labels_map, collapsed_groups_report\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Circular phylogram plotting\n",
    "# ----------------------------------------------------------------------\n",
    "def _hierarchy_to_children(linkage_matrix, labels):\n",
    "    \"\"\"\n",
    "    Build adjacency (children) list from scipy linkage matrix.\n",
    "    Leaf node ids: 0..(n-1)\n",
    "    Internal node ids: n..(n+linkage_matrix.shape[0]-1)\n",
    "    \"\"\"\n",
    "    n_leaves = len(labels)\n",
    "    children = {}\n",
    "    for i, (c1, c2, dist, _) in enumerate(linkage_matrix):\n",
    "        node_id = n_leaves + i\n",
    "        children[node_id] = (int(c1), int(c2))\n",
    "    node_ids = set(range(n_leaves)) | set(children.keys())\n",
    "    return children, node_ids\n",
    "\n",
    "\n",
    "def _gather_descendant_leaves(node_id, children, n_leaves, cache=None):\n",
    "    \"\"\"\n",
    "    Return list of leaf node indices descending from node_id.\n",
    "    \"\"\"\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    if node_id in cache:\n",
    "        return cache[node_id]\n",
    "\n",
    "    if node_id < n_leaves:\n",
    "        cache[node_id] = [node_id]\n",
    "        return cache[node_id]\n",
    "\n",
    "    c1, c2 = children[node_id]\n",
    "    leaves = _gather_descendant_leaves(c1, children, n_leaves, cache) + \\\n",
    "             _gather_descendant_leaves(c2, children, n_leaves, cache)\n",
    "    cache[node_id] = leaves\n",
    "    return leaves\n",
    "\n",
    "\n",
    "def _collect_cluster_colors(cluster_assignments, labels, cmap_name=\"tab20\"):\n",
    "    clusters = sorted(set(cluster_assignments))\n",
    "    cmap = plt.get_cmap(cmap_name, len(clusters))\n",
    "    cluster_to_color = {cid: cmap(i) for i, cid in enumerate(clusters)}\n",
    "    label_to_color = {}\n",
    "    for label_idx, cid in enumerate(cluster_assignments):\n",
    "        label_to_color[labels[label_idx]] = cluster_to_color[cid]\n",
    "    return cluster_to_color, label_to_color\n",
    "\n",
    "\n",
    "def _compute_dual_scale_radii(\n",
    "    feature_matrix, \n",
    "    linkage_matrix, \n",
    "    labels, \n",
    "    inner_boundary=0.6, \n",
    "    consensus_threshold=0.25,\n",
    "    leaf_stretch_power=1.5\n",
    "):\n",
    "    n_leaves = len(labels)\n",
    "    n_internal = linkage_matrix.shape[0]\n",
    "    \n",
    "    # 1. Consensus Ancestor Logic\n",
    "    col_counts = np.array((feature_matrix > 0).sum(axis=0)).flatten()\n",
    "    threshold = n_leaves * consensus_threshold\n",
    "    mca_vec = (col_counts >= threshold).astype(np.int8)\n",
    "    \n",
    "    leaf_div = np.zeros(n_leaves)\n",
    "    for i in range(n_leaves):\n",
    "        leaf_vec = feature_matrix[i].toarray().flatten().astype(np.int8)\n",
    "        intersection = np.sum(np.logical_and(leaf_vec, mca_vec))\n",
    "        union = np.sum(np.logical_or(leaf_vec, mca_vec))\n",
    "        leaf_div[i] = 1.0 - (intersection / union) if union != 0 else 1.0\n",
    "\n",
    "    max_div = np.max(leaf_div) if np.max(leaf_div) > 0 else 1.0\n",
    "    norm_leaf_div = leaf_div / max_div\n",
    "\n",
    "    # 2. Tree Scale (Inner branches remain linear relative to height)\n",
    "    max_h = linkage_matrix[-1, 2] if n_internal > 0 else 1.0\n",
    "    final_radii = {}\n",
    "    for i in range(n_internal):\n",
    "        node_id = n_leaves + i\n",
    "        h = linkage_matrix[i, 2]\n",
    "        progress = 1.0 - (h / max_h) \n",
    "        final_radii[node_id] = progress * inner_boundary\n",
    "\n",
    "    # 3. Leaf Scale (Non-linear Stretch)\n",
    "    children, _ = _hierarchy_to_children(linkage_matrix, labels)\n",
    "    parent_map = {c1: p_id for p_id, (c1, c2) in children.items()}\n",
    "    parent_map.update({c2: p_id for p_id, (c1, c2) in children.items()})\n",
    "\n",
    "    for i in range(n_leaves):\n",
    "        parent_id = parent_map.get(i)\n",
    "        parent_r = final_radii[parent_id] if parent_id is not None else 0\n",
    "        \n",
    "        # Apply the Non-Linear Power Scale here\n",
    "        # This only affects the distance from inner_boundary to 1.0\n",
    "        stretched_div = norm_leaf_div[i] ** leaf_stretch_power\n",
    "        \n",
    "        proposed_r = inner_boundary + (stretched_div * (1.0 - inner_boundary))\n",
    "        final_radii[i] = max(proposed_r, parent_r + 0.01)\n",
    "\n",
    "    return final_radii\n",
    "\n",
    "\n",
    "def _compute_tree_metrics(linkage_matrix, labels):\n",
    "    \"\"\"\n",
    "    Computes root-to-node distances and subtree weights.\n",
    "    Returns:\n",
    "      node_root_dist: dict of {node_id: distance_from_root}\n",
    "      subtree_weight: dict of {node_id: sum_of_internal_distances} (used for angular spacing)\n",
    "    \"\"\"\n",
    "    n_leaves = len(labels)\n",
    "    n_total = n_leaves + linkage_matrix.shape[0]\n",
    "    children, node_ids = _hierarchy_to_children(linkage_matrix, labels)\n",
    "    \n",
    "    # Height of nodes from linkage (distance at which clusters merge)\n",
    "    # heights[leaf] = 0; heights[root] = max_dist\n",
    "    heights = {i: 0.0 for i in range(n_leaves)}\n",
    "    for i, (_, _, dist, _) in enumerate(linkage_matrix):\n",
    "        heights[n_leaves + i] = float(dist)\n",
    "\n",
    "    root_id = n_total - 1\n",
    "    node_root_dist = {root_id: 0.0}\n",
    "    subtree_weight = {i: 1.0 for i in range(n_leaves)} # Base weight for leaves\n",
    "\n",
    "    # Breadth-first or Depth-first to get root-to-node distances\n",
    "    # Distance = parent_height - child_height is the standard branch length\n",
    "    stack = [root_id]\n",
    "    while stack:\n",
    "        curr = stack.pop()\n",
    "        if curr in children:\n",
    "            c1, c2 = children[curr]\n",
    "            # Branch length is the difference in linkage 'age'\n",
    "            branch_l1 = heights[curr] - heights[c1]\n",
    "            branch_l2 = heights[curr] - heights[c2]\n",
    "            \n",
    "            node_root_dist[c1] = node_root_dist[curr] + branch_l1\n",
    "            node_root_dist[c2] = node_root_dist[curr] + branch_l2\n",
    "            \n",
    "            stack.extend([c1, c2])\n",
    "\n",
    "    # Compute subtree weights (sum of distances) to drive angular distribution\n",
    "    # We do this bottom-up\n",
    "    for i in range(linkage_matrix.shape[0]):\n",
    "        node_id = n_leaves + i\n",
    "        c1, c2 = children[node_id]\n",
    "        # Weight is proportional to the distance of the split + children weights\n",
    "        subtree_weight[node_id] = subtree_weight[c1] + subtree_weight[c2] + linkage_matrix[i, 2]\n",
    "\n",
    "    return node_root_dist, subtree_weight, heights\n",
    "\n",
    "\n",
    "def _compute_cluster_aware_angles(linkage_matrix, labels, subtree_weight, gap_factor=0.05):\n",
    "    \"\"\"\n",
    "    Assigns angles with explicit gaps between branches to separate clusters.\n",
    "    gap_factor: percentage of the available arc to leave empty at each split.\n",
    "    \"\"\"\n",
    "    n_leaves = len(labels)\n",
    "    children, _ = _hierarchy_to_children(linkage_matrix, labels)\n",
    "    root_id = n_leaves + linkage_matrix.shape[0] - 1\n",
    "    angles = {}\n",
    "\n",
    "    def assign_angle(node_id, theta_start, theta_end):\n",
    "        if node_id < n_leaves:\n",
    "            angles[node_id] = (theta_start + theta_end) / 2.0\n",
    "            return\n",
    "\n",
    "        c1, c2 = children[node_id]\n",
    "        w1, w2 = subtree_weight[c1], subtree_weight[c2]\n",
    "        \n",
    "        # Calculate available arc after removing a gap\n",
    "        total_arc = theta_end - theta_start\n",
    "        current_gap = total_arc * gap_factor\n",
    "        usable_arc = total_arc - current_gap\n",
    "        \n",
    "        # Split usable arc proportional to weights\n",
    "        mid_point = theta_start + usable_arc * (w1 / (w1 + w2))\n",
    "        \n",
    "        # Branch 1 gets first part, Branch 2 starts after the gap\n",
    "        assign_angle(c1, theta_start, mid_point)\n",
    "        assign_angle(c2, mid_point + current_gap, theta_end)\n",
    "\n",
    "    assign_angle(root_id, 0.0, 2 * np.pi * 0.95) # Leave a final gap at the end\n",
    "    return angles\n",
    "\n",
    "\n",
    "def _draw_circular_tree_with_clusters(\n",
    "    linkage_matrix,\n",
    "    leaf_labels,\n",
    "    cluster_assignments,\n",
    "    label_to_cluster_color,\n",
    "    original_input_sequence_id=None,\n",
    "    labels_map=None,\n",
    "    title=None,\n",
    "    out_prefix=None,\n",
    "    output_dir=\".\",\n",
    "    output_formats=(\"pdf\",),\n",
    "    dpi=600,\n",
    "    show=False,\n",
    "    feature_matrix=None,\n",
    "    inner_boundary=0.5,    # Where tree ends\n",
    "    consensus_threshold=0.25,     # If Features are in 25% of NHs they are \"Ancestral\"\n",
    "    leaf_stretch_power=1.5, # Exaggerate leaf distances non-linearly\n",
    "    gap_factor=0.08\n",
    "):\n",
    "    n_leaves = len(leaf_labels)\n",
    "    \n",
    "    # 1. Radii & Angles\n",
    "    radii = _compute_dual_scale_radii(\n",
    "        feature_matrix, linkage_matrix, leaf_labels, \n",
    "        inner_boundary=inner_boundary, consensus_threshold=consensus_threshold, leaf_stretch_power=leaf_stretch_power\n",
    "    )\n",
    "    _, subtree_weight, _ = _compute_tree_metrics(linkage_matrix, leaf_labels)\n",
    "    angles = _compute_cluster_aware_angles(linkage_matrix, leaf_labels, subtree_weight, gap_factor)\n",
    "    children, _ = _hierarchy_to_children(linkage_matrix, leaf_labels)\n",
    "    \n",
    "    # Coordinate mapping (Angles for internal nodes)\n",
    "    coords = {}\n",
    "    def get_coords(node_id):\n",
    "        if node_id in coords: return coords[node_id]\n",
    "        r = radii[node_id]\n",
    "        if node_id < n_leaves:\n",
    "            theta = angles[node_id]\n",
    "        else:\n",
    "            c1, c2 = children[node_id]\n",
    "            _, th1 = get_coords(c1); _, th2 = get_coords(c2)\n",
    "            theta = (th1 * subtree_weight[c1] + th2 * subtree_weight[c2]) / (subtree_weight[c1] + subtree_weight[c2])\n",
    "        coords[node_id] = (r, theta)\n",
    "        return coords[node_id]\n",
    "\n",
    "    for nid in range(n_leaves + linkage_matrix.shape[0]): get_coords(nid)\n",
    "\n",
    "    # Publication-quality figure setup\n",
    "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"polar\"}, figsize=(20, 20))\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2.0)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # Set publication-ready style parameters\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']\n",
    "    plt.rcParams['pdf.fonttype'] = 42  # TrueType fonts for publication\n",
    "    plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "    # 2. Draw Branches with publication-quality thickness\n",
    "    for node_id, (c1, c2) in children.items():\n",
    "        r_p, th_p = coords[node_id]\n",
    "        for child in (c1, c2):\n",
    "            r_c, th_c = coords[child]\n",
    "            \n",
    "            # Default color with better contrast\n",
    "            color = label_to_cluster_color[leaf_labels[child]] if child < n_leaves else (0.3, 0.3, 0.3, 0.6)\n",
    "            linewidth = 2.0  # Thicker for publication\n",
    "            zorder = 2\n",
    "\n",
    "            # HIGHLIGHT LOGIC: If this branch leads directly to our target\n",
    "            if child < n_leaves and labels_map:\n",
    "                # labels_map[leaf_label] = (org, hit_id, ssn, accession, collapsed)\n",
    "                acc = labels_map[leaf_labels[child]][3]\n",
    "                if original_input_sequence_id and acc == original_input_sequence_id:\n",
    "                    color = \"#DC143C\"  # Crimson for better publication contrast\n",
    "                    linewidth = 3.0  # Much thicker for highlight\n",
    "                    zorder = 10\n",
    "            \n",
    "            # Radial branch with smooth rendering\n",
    "            ax.plot([th_c, th_c], [r_p, r_c], color=color, lw=linewidth, \n",
    "                    zorder=zorder, solid_capstyle='round', solid_joinstyle='round')\n",
    "            \n",
    "            # Connecting arc\n",
    "            t_vals = np.linspace(min(th_p, th_c), max(th_p, th_c), 50)\n",
    "            arc_color = color if child < n_leaves and zorder == 10 else (0.3, 0.3, 0.3, 0.6)\n",
    "            ax.plot(t_vals, np.full_like(t_vals, r_p), color=arc_color, \n",
    "                    lw=linewidth * 0.75, zorder=zorder-1, solid_capstyle='round')\n",
    "\n",
    "    # 3. Draw Leaves & Labels with improved positioning using zigzag stacking\n",
    "    base_label_offset = 0.04  # Base offset for label separation from markers\n",
    "    min_label_radius = 0.7  # Minimum radius for labels to ensure they're on the outside\n",
    "    max_label_offset = 0.15  # Maximum radial offset to prevent labels from going too far\n",
    "    zigzag_spacing = 0.04  # Spacing between zigzag levels\n",
    "    \n",
    "    # Pre-calculate all leaf positions and group by angular proximity\n",
    "    leaf_data = []\n",
    "    for i in range(n_leaves):\n",
    "        r, th = coords[i]\n",
    "        leaf_data.append({'idx': i, 'r': r, 'th': th, 'th_deg': np.degrees(th) % 360})\n",
    "    \n",
    "    # Sort by angle for grouping\n",
    "    leaf_data_sorted = sorted(leaf_data, key=lambda x: x['th'])\n",
    "    \n",
    "    # Group leaves that are close in angle (within ~12 degrees)\n",
    "    angular_group_threshold = np.radians(12)\n",
    "    groups = []\n",
    "    current_group = [leaf_data_sorted[0]]\n",
    "    \n",
    "    for i in range(1, len(leaf_data_sorted)):\n",
    "        prev_leaf = leaf_data_sorted[i-1]\n",
    "        curr_leaf = leaf_data_sorted[i]\n",
    "        \n",
    "        # Calculate angular distance\n",
    "        ang_diff = abs(curr_leaf['th'] - prev_leaf['th'])\n",
    "        if ang_diff > np.pi:\n",
    "            ang_diff = 2 * np.pi - ang_diff\n",
    "        \n",
    "        if ang_diff < angular_group_threshold:\n",
    "            current_group.append(curr_leaf)\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [curr_leaf]\n",
    "    \n",
    "    groups.append(current_group)\n",
    "    \n",
    "    # Calculate label positions with zigzag stacking for each group\n",
    "    label_positions = {}\n",
    "    \n",
    "    for group in groups:\n",
    "        if len(group) == 1:\n",
    "            # Single leaf - simple positioning\n",
    "            leaf = group[0]\n",
    "            offset = base_label_offset\n",
    "            label_r = max(leaf['r'] + offset, min_label_radius)\n",
    "            label_positions[leaf['idx']] = {'r': label_r, 'th': leaf['th']}\n",
    "        else:\n",
    "            # Multiple leaves - zigzag stacking\n",
    "            # Sort by radial distance within group\n",
    "            group_sorted = sorted(group, key=lambda x: x['r'])\n",
    "            \n",
    "            for j, leaf in enumerate(group_sorted):\n",
    "                # Alternate between closer and farther positions (zigzag)\n",
    "                zigzag_level = j % 3  # Use 3 levels: 0, 1, 2\n",
    "                offset = base_label_offset + (zigzag_level * zigzag_spacing)\n",
    "                offset = min(offset, max_label_offset)  # Cap the offset\n",
    "                \n",
    "                label_r = max(leaf['r'] + offset, min_label_radius)\n",
    "                # Keep angle unchanged - no angular adjustment\n",
    "                label_positions[leaf['idx']] = {'r': label_r, 'th': leaf['th']}\n",
    "    \n",
    "    # Now draw leaves and labels using calculated positions\n",
    "    for i in range(n_leaves):\n",
    "        r, th = coords[i]\n",
    "        internal_label = leaf_labels[i]  # Internal key (organism_hit_id)\n",
    "        \n",
    "        # Extract readable label components from labels_map\n",
    "        if labels_map and internal_label in labels_map:\n",
    "            organism, hit_id, ssn_id, accession_id, collapsed_info = labels_map[internal_label]\n",
    "            # Create display label: Organism + Accession ID\n",
    "            label_text = f\"{organism}_{accession_id}\" if accession_id else internal_label\n",
    "        else:\n",
    "            label_text = internal_label\n",
    "        \n",
    "        color = label_to_cluster_color[internal_label]\n",
    "        size = 50  # Larger markers\n",
    "        edge_w = 1.0  # Thicker edges\n",
    "        \n",
    "        # Check for highlight\n",
    "        is_highlight = False\n",
    "        if labels_map and original_input_sequence_id:\n",
    "            acc = labels_map[internal_label][3]\n",
    "            if acc == original_input_sequence_id:\n",
    "                is_highlight = True\n",
    "                color = \"#DC143C\"  # Crimson\n",
    "                size = 180  # Much larger for prominence\n",
    "                edge_w = 2.5  # Thicker edge\n",
    "\n",
    "        # Draw leaf node with publication-quality styling\n",
    "        ax.scatter([th], [r], color=color, s=size, edgecolors='black', \n",
    "                   linewidths=edge_w, zorder=20, alpha=0.9)\n",
    "        \n",
    "        # Get pre-calculated label position\n",
    "        label_r = label_positions[i]['r']\n",
    "        label_th = label_positions[i]['th']\n",
    "        th_deg = np.degrees(label_th) % 360\n",
    "        \n",
    "        # Determine text alignment based on position\n",
    "        # All labels are positioned OUTSIDE (radially outward from) the circle\n",
    "        # Right side (315-45째): left-aligned, labels go to the right\n",
    "        # Top (45-135째): center-aligned, labels go upward  \n",
    "        # Left side (135-225째): right-aligned, labels go to the left\n",
    "        # Bottom (225-315째): center-aligned, labels go downward\n",
    "        \n",
    "        if 315 <= th_deg or th_deg < 45:\n",
    "            ha = 'left'\n",
    "            va = 'center'\n",
    "        elif 45 <= th_deg < 135:\n",
    "            ha = 'center'\n",
    "            va = 'bottom'\n",
    "        elif 135 <= th_deg < 225:\n",
    "            ha = 'right'\n",
    "            va = 'center'\n",
    "        else:\n",
    "            ha = 'center'\n",
    "            va = 'top'\n",
    "        \n",
    "        # Always keep text horizontal for maximum readability\n",
    "        ax.text(label_th, label_r, label_text, \n",
    "                fontsize=15 if not is_highlight else 16,  # Larger fonts for publication\n",
    "                rotation=0,  # Keep horizontal\n",
    "                ha=ha, \n",
    "                va=va, \n",
    "                fontweight='bold' if is_highlight else 'normal',\n",
    "                color=\"#DC143C\" if is_highlight else \"black\",\n",
    "                bbox=dict(boxstyle='round,pad=0.4', \n",
    "                         facecolor='white' if is_highlight else 'white',\n",
    "                         edgecolor='#DC143C' if is_highlight else 'none',\n",
    "                         alpha=0.85 if is_highlight else 0.75,\n",
    "                         linewidth=1.5 if is_highlight else 0),\n",
    "                zorder=25)\n",
    "\n",
    "    # Adjust plot limits to accommodate labels pushed further outward\n",
    "    ax.set_ylim(0, 1.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    if out_prefix and SAVE_PLOTS:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        for fmt in output_formats:\n",
    "            fig.savefig(\n",
    "                os.path.join(output_dir, f\"{out_prefix}_circular_tree.{fmt}\"),\n",
    "                dpi=dpi,\n",
    "                bbox_inches=\"tight\",\n",
    "                facecolor='white',\n",
    "                edgecolor='none'\n",
    "            )\n",
    "        plt.close(fig)\n",
    "    elif show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "                       \n",
    "                \n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Dynamic clustering methods\n",
    "# ----------------------------------------------------------------------\n",
    "def _find_optimal_clusters_by_gap(linkage_matrix, min_clusters=2, min_gap_ratio=1.5):\n",
    "    \"\"\"\n",
    "    Find optimal number of clusters by detecting large gaps in merge heights.\n",
    "    Looks for the largest relative jump in the dendrogram.\n",
    "    \n",
    "    Parameters:\n",
    "    - linkage_matrix: scipy linkage matrix\n",
    "    - min_clusters: minimum number of clusters to consider\n",
    "    - min_gap_ratio: minimum ratio between consecutive heights to consider it a significant gap\n",
    "    \n",
    "    Returns:\n",
    "    - optimal_distance: distance threshold for cutting the dendrogram\n",
    "    - num_clusters: estimated number of clusters\n",
    "    \"\"\"\n",
    "    if linkage_matrix.shape[0] < 2:\n",
    "        return 0.5, 1\n",
    "    \n",
    "    # Extract merge heights (distances at which clusters merge)\n",
    "    heights = linkage_matrix[:, 2]\n",
    "    \n",
    "    # Calculate gaps between consecutive merges\n",
    "    gaps = np.diff(heights)\n",
    "    gap_ratios = gaps[1:] / (gaps[:-1] + 1e-10)  # Avoid division by zero\n",
    "    \n",
    "    # Find the largest gap that produces at least min_clusters\n",
    "    # Start from the end (highest merges = fewest clusters)\n",
    "    max_gap_ratio = -np.inf\n",
    "    best_idx = len(heights) - 1\n",
    "    \n",
    "    for i in range(len(heights) - min_clusters, 0, -1):\n",
    "        if i < len(gap_ratios):\n",
    "            if gap_ratios[i] > max_gap_ratio and gap_ratios[i] >= min_gap_ratio:\n",
    "                max_gap_ratio = gap_ratios[i]\n",
    "                best_idx = i\n",
    "    \n",
    "    # Cut at the height just before the large gap\n",
    "    optimal_distance = (heights[best_idx] + heights[best_idx + 1]) / 2.0 if best_idx < len(heights) - 1 else heights[best_idx] * 1.1\n",
    "    num_clusters = linkage_matrix.shape[0] - best_idx + 1\n",
    "    \n",
    "    return optimal_distance, num_clusters\n",
    "\n",
    "\n",
    "def _find_optimal_clusters_by_inconsistency(linkage_matrix, depth=2, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Use inconsistency coefficient to find natural clusters.\n",
    "    Inconsistency measures how different a merge is compared to nearby merges.\n",
    "    \n",
    "    Parameters:\n",
    "    - linkage_matrix: scipy linkage matrix\n",
    "    - depth: how many levels to look back for comparison\n",
    "    - threshold: inconsistency threshold for cutting\n",
    "    \n",
    "    Returns:\n",
    "    - cluster_assignments: array of cluster labels\n",
    "    \"\"\"\n",
    "    from scipy.cluster.hierarchy import inconsistent, fcluster\n",
    "    \n",
    "    if linkage_matrix.shape[0] < 2:\n",
    "        return np.ones(linkage_matrix.shape[0] + 1, dtype=int)\n",
    "    \n",
    "    # Calculate inconsistency coefficients\n",
    "    incons = inconsistent(linkage_matrix, d=depth)\n",
    "    \n",
    "    # Find a good threshold based on inconsistency statistics\n",
    "    # Use the mean + threshold * std of inconsistency values\n",
    "    mean_incons = np.mean(incons[:, 3])  # Column 3 is the inconsistency coefficient\n",
    "    std_incons = np.std(incons[:, 3])\n",
    "    \n",
    "    cutoff = mean_incons + threshold * std_incons\n",
    "    \n",
    "    # Use the inconsistency criterion for clustering\n",
    "    cluster_assignments = fcluster(linkage_matrix, cutoff, criterion='inconsistent', depth=depth)\n",
    "    \n",
    "    return cluster_assignments\n",
    "\n",
    "\n",
    "def _find_optimal_clusters_by_elbow(linkage_matrix, max_clusters=None):\n",
    "    \"\"\"\n",
    "    Use the elbow method on within-cluster distances.\n",
    "    Finds the point where adding more clusters doesn't significantly improve separation.\n",
    "    \n",
    "    Parameters:\n",
    "    - linkage_matrix: scipy linkage matrix\n",
    "    - max_clusters: maximum number of clusters to test (default: sqrt(n))\n",
    "    \n",
    "    Returns:\n",
    "    - optimal_distance: distance threshold for cutting\n",
    "    - optimal_k: optimal number of clusters\n",
    "    \"\"\"\n",
    "    from scipy.cluster.hierarchy import fcluster\n",
    "    \n",
    "    n = linkage_matrix.shape[0] + 1  # Number of leaves\n",
    "    \n",
    "    if n < 3:\n",
    "        return linkage_matrix[-1, 2] / 2.0 if linkage_matrix.shape[0] > 0 else 0.5, 1\n",
    "    \n",
    "    if max_clusters is None:\n",
    "        max_clusters = min(int(np.sqrt(n)), n - 1)\n",
    "    \n",
    "    max_clusters = max(2, min(max_clusters, n - 1))\n",
    "    \n",
    "    # Test different numbers of clusters\n",
    "    wcss_values = []  # Within-cluster sum of squares (approximated)\n",
    "    k_range = range(1, max_clusters + 1)\n",
    "    \n",
    "    heights = linkage_matrix[:, 2]\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Height at which we'd get k clusters\n",
    "        if k == 1:\n",
    "            wcss = heights[-1] if len(heights) > 0 else 0\n",
    "        else:\n",
    "            # The height where we cut to get k clusters\n",
    "            idx = -(k - 1) if k <= len(heights) else 0\n",
    "            wcss = heights[idx] if idx < len(heights) else 0\n",
    "        \n",
    "        wcss_values.append(wcss)\n",
    "    \n",
    "    wcss_values = np.array(wcss_values)\n",
    "    \n",
    "    # Find elbow using the method of maximum distance to the line\n",
    "    if len(wcss_values) < 3:\n",
    "        return heights[-1] / 2.0, 2\n",
    "    \n",
    "    # Normalize\n",
    "    x = np.arange(len(wcss_values))\n",
    "    y = wcss_values\n",
    "    \n",
    "    # Line from first to last point\n",
    "    p1 = np.array([x[0], y[0]])\n",
    "    p2 = np.array([x[-1], y[-1]])\n",
    "    \n",
    "    # Calculate distances from each point to the line\n",
    "    distances = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        p = np.array([x[i], y[i]])\n",
    "        distances[i] = np.abs(np.cross(p2 - p1, p1 - p)) / np.linalg.norm(p2 - p1)\n",
    "    \n",
    "    # Find the elbow (maximum distance)\n",
    "    optimal_idx = np.argmax(distances)\n",
    "    optimal_k = k_range[optimal_idx]\n",
    "    \n",
    "    # Find the distance threshold for optimal_k clusters\n",
    "    if optimal_k == 1:\n",
    "        optimal_distance = heights[-1] * 1.1 if len(heights) > 0 else 1.0\n",
    "    else:\n",
    "        cut_idx = -(optimal_k - 1)\n",
    "        if cut_idx >= -len(heights):\n",
    "            optimal_distance = (heights[cut_idx] + heights[cut_idx - 1]) / 2.0 if cut_idx > -len(heights) else heights[cut_idx]\n",
    "        else:\n",
    "            optimal_distance = heights[0] / 2.0\n",
    "    \n",
    "    return optimal_distance, optimal_k\n",
    "\n",
    "\n",
    "def _find_optimal_clusters_by_lifetime(linkage_matrix, percentile=90):\n",
    "    \"\"\"\n",
    "    Use cluster lifetime (persistence) to find stable clusters.\n",
    "    Clusters that exist for a long 'time' (distance range) are considered robust.\n",
    "    \n",
    "    Parameters:\n",
    "    - linkage_matrix: scipy linkage matrix\n",
    "    - percentile: percentile of lifetimes to use as threshold\n",
    "    \n",
    "    Returns:\n",
    "    - optimal_distance: distance threshold\n",
    "    - num_clusters: estimated number of clusters\n",
    "    \"\"\"\n",
    "    if linkage_matrix.shape[0] < 2:\n",
    "        return 0.5, 1\n",
    "    \n",
    "    heights = linkage_matrix[:, 2]\n",
    "    n = linkage_matrix.shape[0] + 1\n",
    "    \n",
    "    # Calculate lifetimes: for each merge, how long did the clusters exist?\n",
    "    # Lifetime = height at which cluster dies - height at which it was born\n",
    "    lifetimes = []\n",
    "    birth_times = np.zeros(2 * n - 1)  # Birth time for each node\n",
    "    \n",
    "    for i in range(len(heights)):\n",
    "        c1, c2 = int(linkage_matrix[i, 0]), int(linkage_matrix[i, 1])\n",
    "        merge_height = heights[i]\n",
    "        new_cluster = n + i\n",
    "        \n",
    "        # Lifetime of clusters being merged\n",
    "        life1 = merge_height - birth_times[c1]\n",
    "        life2 = merge_height - birth_times[c2]\n",
    "        \n",
    "        lifetimes.append(life1)\n",
    "        lifetimes.append(life2)\n",
    "        \n",
    "        # New cluster born at this height\n",
    "        birth_times[new_cluster] = merge_height\n",
    "    \n",
    "    lifetimes = np.array(lifetimes)\n",
    "    \n",
    "    # Use percentile of lifetimes as threshold\n",
    "    significant_lifetime = np.percentile(lifetimes, percentile)\n",
    "    \n",
    "    # Find merges where both clusters have long lifetimes\n",
    "    cut_height = None\n",
    "    for i in range(len(heights)):\n",
    "        c1, c2 = int(linkage_matrix[i, 0]), int(linkage_matrix[i, 1])\n",
    "        merge_height = heights[i]\n",
    "        \n",
    "        life1 = merge_height - birth_times[c1]\n",
    "        life2 = merge_height - birth_times[c2]\n",
    "        \n",
    "        if life1 >= significant_lifetime or life2 >= significant_lifetime:\n",
    "            cut_height = merge_height\n",
    "            break\n",
    "    \n",
    "    if cut_height is None:\n",
    "        cut_height = np.median(heights)\n",
    "    \n",
    "    # Estimate number of clusters at this cut\n",
    "    num_clusters = np.sum(heights > cut_height) + 1\n",
    "    \n",
    "    return cut_height, num_clusters\n",
    "\n",
    "\n",
    "def _find_optimal_clusters_by_topology(linkage_matrix, max_clusters=None, coherence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Find clusters based on tree topology to ensure visual coherence.\n",
    "    Groups leaves that share recent common ancestors and form complete subtrees.\n",
    "    \n",
    "    Parameters:\n",
    "    - linkage_matrix: scipy linkage matrix\n",
    "    - max_clusters: maximum number of clusters (default: sqrt(n))\n",
    "    - coherence_threshold: how coherent subtrees should be (0-1, higher = more coherent)\n",
    "    \n",
    "    Returns:\n",
    "    - optimal_distance: distance threshold\n",
    "    - num_clusters: number of clusters\n",
    "    \"\"\"\n",
    "    from scipy.cluster.hierarchy import fcluster\n",
    "    \n",
    "    n = linkage_matrix.shape[0] + 1  # Number of leaves\n",
    "    \n",
    "    if n < 3:\n",
    "        return linkage_matrix[-1, 2] / 2.0 if linkage_matrix.shape[0] > 0 else 0.5, 1\n",
    "    \n",
    "    if max_clusters is None:\n",
    "        max_clusters = max(2, min(int(np.sqrt(n) * 1.5), n - 1))\n",
    "    \n",
    "    heights = linkage_matrix[:, 2]\n",
    "    \n",
    "    # Build tree structure\n",
    "    children_dict = {}\n",
    "    for i in range(linkage_matrix.shape[0]):\n",
    "        node_id = n + i\n",
    "        children_dict[node_id] = (int(linkage_matrix[i, 0]), int(linkage_matrix[i, 1]))\n",
    "    \n",
    "    def get_descendants(node_id):\n",
    "        \"\"\"Get all leaf descendants of a node\"\"\"\n",
    "        if node_id < n:\n",
    "            return [node_id]\n",
    "        c1, c2 = children_dict[node_id]\n",
    "        return get_descendants(c1) + get_descendants(c2)\n",
    "    \n",
    "    # Evaluate different cut heights for coherence\n",
    "    best_score = -np.inf\n",
    "    best_height = heights[-1] / 2.0\n",
    "    best_k = 2\n",
    "    \n",
    "    # Test different numbers of clusters\n",
    "    for k in range(2, max_clusters + 1):\n",
    "        # Cut tree to get k clusters\n",
    "        if k > len(heights):\n",
    "            continue\n",
    "            \n",
    "        cut_idx = -(k - 1)\n",
    "        if cut_idx < -len(heights):\n",
    "            continue\n",
    "            \n",
    "        cut_height = (heights[cut_idx] + heights[cut_idx - 1]) / 2.0 if cut_idx > -len(heights) else heights[cut_idx]\n",
    "        \n",
    "        # Get cluster assignments\n",
    "        assignments = fcluster(linkage_matrix, cut_height, criterion='distance')\n",
    "        \n",
    "        # Calculate coherence score: how well do clusters correspond to subtrees?\n",
    "        # Look at the tree structure at this cut height\n",
    "        coherence_scores = []\n",
    "        \n",
    "        for cluster_id in np.unique(assignments):\n",
    "            cluster_leaves = np.where(assignments == cluster_id)[0]\n",
    "            if len(cluster_leaves) < 2:\n",
    "                coherence_scores.append(1.0)  # Single leaf is perfectly coherent\n",
    "                continue\n",
    "            \n",
    "            # Find the lowest common ancestor (LCA) height for leaves in this cluster\n",
    "            # A good cluster should have leaves that diverge at similar heights\n",
    "            cluster_heights = []\n",
    "            for i, leaf1 in enumerate(cluster_leaves):\n",
    "                for leaf2 in cluster_leaves[i+1:]:\n",
    "                    # Find LCA height in linkage matrix\n",
    "                    # This is approximate - look for merges involving these leaves\n",
    "                    cluster_heights.append(0.0)  # Simplified for now\n",
    "            \n",
    "            if cluster_heights:\n",
    "                # Coherence is high if all leaves in cluster are close in tree\n",
    "                coherence = 1.0 - np.std(cluster_heights) / (np.mean(heights) + 1e-10)\n",
    "            else:\n",
    "                coherence = 1.0\n",
    "            \n",
    "            coherence_scores.append(coherence)\n",
    "        \n",
    "        # Score combines number of clusters with coherence\n",
    "        avg_coherence = np.mean(coherence_scores)\n",
    "        \n",
    "        # Penalize too few or too many clusters, reward high coherence\n",
    "        k_penalty = abs(k - np.sqrt(n)) / n\n",
    "        score = avg_coherence * (1.0 - k_penalty * 0.5)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_height = cut_height\n",
    "            best_k = k\n",
    "    \n",
    "    return best_height, best_k\n",
    "\n",
    "\n",
    "def _find_optimal_clusters_by_maxclust(linkage_matrix, target_clusters=None):\n",
    "    \"\"\"\n",
    "    Find optimal clustering by specifying target number of clusters.\n",
    "    Uses the maxclust criterion to create exactly k clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    - linkage_matrix: scipy linkage matrix\n",
    "    - target_clusters: desired number of clusters (default: sqrt(n))\n",
    "    \n",
    "    Returns:\n",
    "    - cluster_assignments: array of cluster labels\n",
    "    - num_clusters: actual number of clusters created\n",
    "    \"\"\"\n",
    "    from scipy.cluster.hierarchy import fcluster\n",
    "    \n",
    "    n = linkage_matrix.shape[0] + 1\n",
    "    \n",
    "    if n < 2:\n",
    "        return np.ones(n, dtype=int), 1\n",
    "    \n",
    "    if target_clusters is None:\n",
    "        target_clusters = max(2, int(np.sqrt(n)))\n",
    "    \n",
    "    target_clusters = max(1, min(target_clusters, n))\n",
    "    \n",
    "    # Use maxclust criterion for exact number of clusters\n",
    "    cluster_assignments = fcluster(linkage_matrix, target_clusters, criterion='maxclust')\n",
    "    \n",
    "    return cluster_assignments, target_clusters\n",
    "\n",
    "\n",
    "def determine_optimal_clustering(linkage_matrix, method='gap', **kwargs):\n",
    "    \"\"\"\n",
    "    Determine optimal clustering using various dynamic methods.\n",
    "    \n",
    "    Parameters:\n",
    "    - linkage_matrix: scipy linkage matrix\n",
    "    - method: 'gap', 'inconsistency', 'elbow', 'lifetime', 'topology', 'maxclust', or 'combined'\n",
    "    - **kwargs: additional parameters for specific methods\n",
    "    \n",
    "    Returns:\n",
    "    - cluster_assignments: array of cluster labels\n",
    "    - info: dict with method-specific information\n",
    "    \"\"\"\n",
    "    from scipy.cluster.hierarchy import fcluster\n",
    "    \n",
    "    n = linkage_matrix.shape[0] + 1\n",
    "    \n",
    "    if n < 2:\n",
    "        return np.ones(n, dtype=int), {'method': method, 'n_clusters': 1}\n",
    "    \n",
    "    info = {'method': method}\n",
    "    \n",
    "    if method == 'gap':\n",
    "        distance_threshold, n_clusters = _find_optimal_clusters_by_gap(\n",
    "            linkage_matrix, \n",
    "            min_clusters=kwargs.get('min_clusters', 2),\n",
    "            min_gap_ratio=kwargs.get('min_gap_ratio', 1.5)\n",
    "        )\n",
    "        info['distance_threshold'] = distance_threshold\n",
    "        info['n_clusters'] = n_clusters\n",
    "        cluster_assignments = fcluster(linkage_matrix, distance_threshold, criterion='distance')\n",
    "        \n",
    "    elif method == 'inconsistency':\n",
    "        cluster_assignments = _find_optimal_clusters_by_inconsistency(\n",
    "            linkage_matrix,\n",
    "            depth=kwargs.get('depth', 2),\n",
    "            threshold=kwargs.get('threshold', 1.5)\n",
    "        )\n",
    "        info['n_clusters'] = len(np.unique(cluster_assignments))\n",
    "        \n",
    "    elif method == 'elbow':\n",
    "        distance_threshold, n_clusters = _find_optimal_clusters_by_elbow(\n",
    "            linkage_matrix,\n",
    "            max_clusters=kwargs.get('max_clusters', None)\n",
    "        )\n",
    "        info['distance_threshold'] = distance_threshold\n",
    "        info['n_clusters'] = n_clusters\n",
    "        cluster_assignments = fcluster(linkage_matrix, distance_threshold, criterion='distance')\n",
    "        \n",
    "    elif method == 'lifetime':\n",
    "        distance_threshold, n_clusters = _find_optimal_clusters_by_lifetime(\n",
    "            linkage_matrix,\n",
    "            percentile=kwargs.get('percentile', 90)\n",
    "        )\n",
    "        info['distance_threshold'] = distance_threshold\n",
    "        info['n_clusters'] = n_clusters\n",
    "        cluster_assignments = fcluster(linkage_matrix, distance_threshold, criterion='distance')\n",
    "        \n",
    "    elif method == 'topology':\n",
    "        distance_threshold, n_clusters = _find_optimal_clusters_by_topology(\n",
    "            linkage_matrix,\n",
    "            max_clusters=kwargs.get('max_clusters', None),\n",
    "            coherence_threshold=kwargs.get('coherence_threshold', 0.7)\n",
    "        )\n",
    "        info['distance_threshold'] = distance_threshold\n",
    "        info['n_clusters'] = n_clusters\n",
    "        info['note'] = 'Tree-topology aware clustering for visual coherence'\n",
    "        cluster_assignments = fcluster(linkage_matrix, distance_threshold, criterion='distance')\n",
    "        \n",
    "    elif method == 'maxclust':\n",
    "        cluster_assignments, n_clusters = _find_optimal_clusters_by_maxclust(\n",
    "            linkage_matrix,\n",
    "            target_clusters=kwargs.get('target_clusters', None)\n",
    "        )\n",
    "        info['n_clusters'] = n_clusters\n",
    "        info['note'] = f'Forced {n_clusters} clusters using maxclust criterion'\n",
    "        \n",
    "    elif method == 'combined':\n",
    "        # Use multiple methods and take consensus\n",
    "        methods_to_try = ['gap', 'elbow', 'topology']\n",
    "        all_n_clusters = []\n",
    "        all_assignments = []\n",
    "        \n",
    "        for m in methods_to_try:\n",
    "            try:\n",
    "                assignments, m_info = determine_optimal_clustering(linkage_matrix, method=m)\n",
    "                all_n_clusters.append(m_info['n_clusters'])\n",
    "                all_assignments.append(assignments)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if all_n_clusters:\n",
    "            # Use median number of clusters\n",
    "            median_k = int(np.median(all_n_clusters))\n",
    "            info['n_clusters'] = median_k\n",
    "            info['individual_estimates'] = all_n_clusters\n",
    "            \n",
    "            # Find the assignment closest to median\n",
    "            differences = [abs(len(np.unique(a)) - median_k) for a in all_assignments]\n",
    "            best_idx = np.argmin(differences)\n",
    "            cluster_assignments = all_assignments[best_idx]\n",
    "        else:\n",
    "            # Fallback\n",
    "            cluster_assignments = fcluster(linkage_matrix, 0.7, criterion='distance')\n",
    "            info['n_clusters'] = len(np.unique(cluster_assignments))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    return cluster_assignments, info\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Main clustering function (using circular tree plotting)\n",
    "# ----------------------------------------------------------------------\n",
    "def cluster_gene_neighborhoods_from_sqlite(\n",
    "    db_path,\n",
    "    genes_table=GENES_TABLE,\n",
    "    neighbors_table=NEIGHBORS_TABLE,\n",
    "    col_neighborhood_id=COL_NEIGHBORHOOD_ID,\n",
    "    col_gene_id=COL_GENE_ID,\n",
    "    col_linking_key=COL_LINKING_KEY,\n",
    "    col_accession_id=COL_ACCESSION_ID,\n",
    "    col_function_desc=COL_FUNCTION_DESC,\n",
    "    col_pfam_ids=COL_PFAM_IDS,\n",
    "    col_interpro_ids=COL_INTERPRO_IDS,\n",
    "    col_rel_start=COL_REL_START,\n",
    "    col_rel_stop=COL_REL_STOP,\n",
    "    col_ssn_cluster_id=COL_SSN_CLUSTER_ID,\n",
    "    hit_gene_weight_factor=HIT_GENE_WEIGHT_FACTOR,\n",
    "    direct_neighbor_weight_factor=DIRECT_NEIGHBOR_WEIGHT_FACTOR,\n",
    "    differentiate_by_ssn_cluster=True,\n",
    "    ssn_cluster_value_to_filter=DEFAULT_SSN_CLUSTER_VALUE_TO_FILTER,\n",
    "    collapse_identical_neighborhoods=COLLAPSE_IDENTICAL_NEIGHBORHOODS,\n",
    "    collapse_core_similarity_threshold=COLLAPSE_CORE_SIMILARITY_THRESHOLD,\n",
    "    collapse_full_neighborhood_similarity_threshold=COLLAPSE_FULL_NEIGHBORHOOD_SIMILARITY_THRESHOLD,\n",
    "    original_input_sequence_id=None,\n",
    "    distance_threshold=0.8,\n",
    "    clustering_method='gap',\n",
    "    clustering_params=None,\n",
    "    save_plots=SAVE_PLOTS,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    output_formats=OUTPUT_FORMATS,\n",
    "    dpi=DPI,\n",
    "    report_file_handle=None,\n",
    "    parallelize_pdist=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cluster gene neighborhoods based on domain/annotation features and\n",
    "    draw circular phylograms for each SSN cluster (or all neighborhoods).\n",
    "    \n",
    "    Clustering methods:\n",
    "    - 'gap': Finds large gaps in merge heights (fast, good for well-separated clusters)\n",
    "    - 'inconsistency': Uses inconsistency coefficients (hierarchical structure aware)\n",
    "    - 'elbow': Elbow method on within-cluster distances (classic approach)\n",
    "    - 'lifetime': Cluster persistence/lifetime analysis (finds stable clusters)\n",
    "    - 'topology': Tree-topology aware (best for visual coherence) **RECOMMENDED**\n",
    "    - 'maxclust': Forces specific number of clusters (when you know k)\n",
    "    - 'combined': Consensus of multiple methods (most robust but slower)\n",
    "    - 'static': Use fixed distance_threshold (legacy)\n",
    "    \"\"\"\n",
    "    def log(msg):\n",
    "        print(msg)\n",
    "        if report_file_handle:\n",
    "            report_file_handle.write(msg + \"\\n\")\n",
    "\n",
    "    t0_all = time.time()\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # --- Fetch data ---\n",
    "    log(\"Fetching hit gene data ...\")\n",
    "    q_hits = f\"\"\"\n",
    "        SELECT {col_gene_id}, {col_neighborhood_id}, {col_function_desc},\n",
    "               {col_pfam_ids}, {col_interpro_ids}, {col_ssn_cluster_id},\n",
    "               {col_accession_id}\n",
    "        FROM {genes_table}\n",
    "    \"\"\"\n",
    "    hit_genes_df = pd.read_sql_query(q_hits, conn)\n",
    "    log(f\"  Retrieved {len(hit_genes_df)} hit genes\")\n",
    "\n",
    "    if hit_genes_df.empty:\n",
    "        log(\"No hit genes found; aborting.\")\n",
    "        conn.close()\n",
    "        return {}, {}, {}\n",
    "\n",
    "    log(\"Fetching neighbor data ...\")\n",
    "    q_neighbors = f\"\"\"\n",
    "        SELECT {col_linking_key}, {col_gene_id}, {col_function_desc},\n",
    "               {col_pfam_ids}, {col_interpro_ids},\n",
    "               {col_rel_start}, {col_rel_stop}\n",
    "        FROM {neighbors_table}\n",
    "    \"\"\"\n",
    "    neighbors_df = pd.read_sql_query(q_neighbors, conn)\n",
    "    conn.close()\n",
    "    log(f\"  Retrieved {len(neighbors_df)} neighbor rows\")\n",
    "\n",
    "    # Group neighbors by linking key once\n",
    "    log(\"Grouping neighbors by hit gene id ...\")\n",
    "    neighbors_by_link_id = {\n",
    "        key: group.to_dict(\"records\")\n",
    "        for key, group in tqdm(\n",
    "            neighbors_df.groupby(col_linking_key),\n",
    "            desc=\"  Grouping neighbors\",\n",
    "            leave=False,\n",
    "        )\n",
    "    }\n",
    "    del neighbors_df\n",
    "    gc.collect()\n",
    "\n",
    "    all_neighborhood_features = defaultdict(set)\n",
    "    core_neighborhood_features = defaultdict(set)\n",
    "    full_neighborhood_labels_map = {}\n",
    "    raw_ssn_counts = defaultdict(int)\n",
    "\n",
    "    # --- Feature extraction ---\n",
    "    log(f\"Extracting features for {len(hit_genes_df)} neighborhoods ...\")\n",
    "    for _, hit_row in tqdm(\n",
    "        hit_genes_df.iterrows(),\n",
    "        total=len(hit_genes_df),\n",
    "        desc=\"  Neighborhoods\",\n",
    "        unit=\"nh\",\n",
    "    ):\n",
    "        hit_id = hit_row[col_gene_id]\n",
    "        organism = hit_row[col_neighborhood_id]\n",
    "        ssn_id = hit_row[col_ssn_cluster_id]\n",
    "        accession_id = hit_row[col_accession_id]\n",
    "\n",
    "        raw_ssn_counts[ssn_id] += 1\n",
    "        nh_label = f\"{organism}_{hit_id}\"\n",
    "\n",
    "        full_feats = set()\n",
    "        core_feats = set()\n",
    "\n",
    "        # hit gene, strongly weighted for full neighborhood features\n",
    "        hit_full = extract_features_from_gene_row(\n",
    "            hit_row,\n",
    "            current_weight_factor=hit_gene_weight_factor,\n",
    "            base_prefix=\"HIT_\",\n",
    "            include_desc=True,\n",
    "            include_pfam=True,\n",
    "            include_interpro=True,\n",
    "        )\n",
    "        full_feats.update(hit_full)\n",
    "\n",
    "        hit_core = extract_features_from_gene_row(\n",
    "            hit_row,\n",
    "            current_weight_factor=1,\n",
    "            base_prefix=\"HIT_CORE_\",\n",
    "            include_desc=False,\n",
    "            include_pfam=True,\n",
    "            include_interpro=True,\n",
    "        )\n",
    "        core_feats.update(hit_core)\n",
    "\n",
    "        full_neighborhood_labels_map[nh_label] = (\n",
    "            organism,\n",
    "            hit_id,\n",
    "            ssn_id,\n",
    "            accession_id,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        # determine closest left/right neighbors by relative coordinates\n",
    "        neighbor_rows = neighbors_by_link_id.get(hit_id, [])\n",
    "        closest_left_id = None\n",
    "        closest_right_id = None\n",
    "        max_neg_rel_stop = -np.inf\n",
    "        min_pos_rel_start = np.inf\n",
    "\n",
    "        for nrow in neighbor_rows:\n",
    "            rel_start = nrow[col_rel_start]\n",
    "            rel_stop = nrow[col_rel_stop]\n",
    "            nid = nrow[col_gene_id]\n",
    "\n",
    "            if rel_stop is not None and rel_stop < 0 and rel_stop > max_neg_rel_stop:\n",
    "                max_neg_rel_stop = rel_stop\n",
    "                closest_left_id = nid\n",
    "            if rel_start is not None and rel_start > 0 and rel_start < min_pos_rel_start:\n",
    "                min_pos_rel_start = rel_start\n",
    "                closest_right_id = nid\n",
    "\n",
    "        for nrow in neighbor_rows:\n",
    "            nid = nrow[col_gene_id]\n",
    "            is_direct = (\n",
    "                (closest_left_id is not None and nid == closest_left_id)\n",
    "                or (closest_right_id is not None and nid == closest_right_id)\n",
    "            )\n",
    "            w = direct_neighbor_weight_factor if is_direct else 1\n",
    "\n",
    "            nf_full = extract_features_from_gene_row(\n",
    "                nrow,\n",
    "                current_weight_factor=w,\n",
    "                base_prefix=\"N_\",\n",
    "                include_desc=True,\n",
    "                include_pfam=True,\n",
    "                include_interpro=True,\n",
    "            )\n",
    "            full_feats.update(nf_full)\n",
    "\n",
    "            if is_direct:\n",
    "                nf_core = extract_features_from_gene_row(\n",
    "                    nrow,\n",
    "                    current_weight_factor=1,\n",
    "                    base_prefix=\"N_CORE_\",\n",
    "                    include_desc=False,\n",
    "                    include_pfam=True,\n",
    "                    include_interpro=True,\n",
    "                )\n",
    "                core_feats.update(nf_core)\n",
    "\n",
    "        all_neighborhood_features[nh_label] = full_feats\n",
    "        core_neighborhood_features[nh_label] = core_feats\n",
    "\n",
    "    del neighbors_by_link_id\n",
    "    gc.collect()\n",
    "    log(\"Feature extraction complete.\")\n",
    "\n",
    "    log(\"\\nRaw SSN id distribution:\")\n",
    "    for sid, count in sorted(raw_ssn_counts.items(), key=lambda x: str(x[0])):\n",
    "        log(f\"  SSN {sid}: {count} neighborhoods\")\n",
    "    log(\"-\" * 60)\n",
    "\n",
    "    if not all_neighborhood_features:\n",
    "        log(\"No neighborhoods with features; aborting.\")\n",
    "        return {}, {}, {}\n",
    "\n",
    "    # --- Collapsing similar neighborhoods (optional) ---\n",
    "    if collapse_identical_neighborhoods:\n",
    "        final_neighborhood_features, final_neighborhood_labels_map, collapsed_groups_report = _perform_collapsing(\n",
    "            all_neighborhood_features,\n",
    "            full_neighborhood_labels_map,\n",
    "            core_neighborhood_features,\n",
    "            collapse_core_similarity_threshold,\n",
    "            collapse_full_neighborhood_similarity_threshold,\n",
    "            output_prefix=\"[Collapsing]\",\n",
    "            report_file=report_file_handle,\n",
    "            parallelize_pdist=parallelize_pdist,\n",
    "        )\n",
    "    else:\n",
    "        log(\"Collapsing disabled; using all neighborhoods as-is.\")\n",
    "        final_neighborhood_features = all_neighborhood_features\n",
    "        final_neighborhood_labels_map = full_neighborhood_labels_map\n",
    "        collapsed_groups_report = {}\n",
    "\n",
    "    del all_neighborhood_features\n",
    "    del core_neighborhood_features\n",
    "    gc.collect()\n",
    "\n",
    "    # sanity check on features\n",
    "    if not final_neighborhood_features:\n",
    "        log(\"No final neighborhoods after collapsing; aborting.\")\n",
    "        return {}, {}, collapsed_groups_report\n",
    "\n",
    "    # determine grouping (SSN-separated or all)\n",
    "    ssn_groups = defaultdict(list)\n",
    "    if differentiate_by_ssn_cluster:\n",
    "        for nh_label, (_, _, ssn_id, _, _) in final_neighborhood_labels_map.items():\n",
    "            if ssn_id not in ssn_cluster_value_to_filter:\n",
    "                ssn_groups[ssn_id].append(nh_label)\n",
    "        log(f\"\\nProcessing {len(ssn_groups)} SSN groups (after filtering).\")\n",
    "        if ssn_groups:\n",
    "            log(\"  SSN ids: \" + \", \".join(map(str, sorted(ssn_groups.keys(), key=str))))\n",
    "    else:\n",
    "        ssn_groups[\"All_Neighborhoods\"] = sorted(final_neighborhood_features.keys())\n",
    "        log(\"Processing all neighborhoods together (no SSN separation).\")\n",
    "\n",
    "    clusters_output = defaultdict(dict)\n",
    "    if differentiate_by_ssn_cluster and not ssn_groups:\n",
    "        log(\"No valid SSN clusters with neighborhoods; nothing to cluster.\")\n",
    "        return {}, final_neighborhood_labels_map, collapsed_groups_report\n",
    "\n",
    "    # --- Per-group clustering and plotting ---\n",
    "    for ssn_id, nh_labels in tqdm(\n",
    "        ssn_groups.items(),\n",
    "        desc=\"Clustering SSN groups\",\n",
    "        unit=\"group\",\n",
    "    ):\n",
    "        group_start = time.time()\n",
    "        label_list = sorted(nh_labels)\n",
    "        n_nh = len(label_list)\n",
    "        if n_nh < 2:\n",
    "            log(f\"SSN {ssn_id}: only {n_nh} neighborhood(s); skipping clustering.\")\n",
    "            clusters_output[ssn_id] = {1: label_list}\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing SSN {ssn_id} with {len(label_list)} neighborhoods.\")\n",
    "        \n",
    "        log(f\"\\n--- SSN {ssn_id}: {n_nh} neighborhoods ---\")\n",
    "        group_feats = {l: final_neighborhood_features[l] for l in label_list}\n",
    "        group_labels_map = {l: final_neighborhood_labels_map[l] for l in label_list}\n",
    "\n",
    "        # vocabulary and binary matrix\n",
    "        vocab = sorted(set.union(*group_feats.values()))\n",
    "        if not vocab:\n",
    "            log(f\"  SSN {ssn_id}: no features; skipping.\")\n",
    "            clusters_output[ssn_id] = {1: label_list}\n",
    "            continue\n",
    "\n",
    "        ft_idx = {f: i for i, f in enumerate(vocab)}\n",
    "        mat_lil = sp.lil_matrix((n_nh, len(vocab)), dtype=np.int8)\n",
    "        for i, nh in enumerate(tqdm(label_list, desc=f\"  Features SSN {ssn_id}\", leave=False)):\n",
    "            for f in group_feats[nh]:\n",
    "                j = ft_idx[f]\n",
    "                mat_lil[i, j] = 1\n",
    "        mat = mat_lil.tocsr()\n",
    "        del mat_lil\n",
    "        gc.collect()\n",
    "\n",
    "        # all identical?\n",
    "        if n_nh > 1 and all((mat[0] != mat[i]).nnz == 0 for i in range(1, n_nh)):\n",
    "            log(f\"  SSN {ssn_id}: all neighborhoods identical; no tree/distance.\")\n",
    "            clusters_output[ssn_id] = {1: label_list}\n",
    "            del mat\n",
    "            gc.collect()\n",
    "            continue\n",
    "\n",
    "        # distances + linkage\n",
    "        log(f\"  SSN {ssn_id}: computing Jaccard distances ...\")\n",
    "        t0 = time.time()\n",
    "        dists = parallel_pdist_jaccard(mat, num_cores=-1 if parallelize_pdist else 1)\n",
    "        log(f\"    distance calc in {time.time() - t0:.2f}s\")\n",
    "\n",
    "        log(f\"  SSN {ssn_id}: linkage ...\")\n",
    "        t0 = time.time()\n",
    "        linked = linkage(dists, method=\"average\")\n",
    "        log(f\"    linkage in {time.time() - t0:.2f}s\")\n",
    "\n",
    "        del dists\n",
    "        gc.collect()\n",
    "\n",
    "        # Dynamic cluster assignments\n",
    "        log(f\"  SSN {ssn_id}: determining optimal clusters (method: {clustering_method}) ...\")\n",
    "        t0 = time.time()\n",
    "        \n",
    "        if clustering_method == 'static':\n",
    "            # Legacy: use fixed threshold\n",
    "            cluster_assignments = fcluster(linked, distance_threshold, criterion=\"distance\")\n",
    "            n_clusters = len(np.unique(cluster_assignments))\n",
    "            log(f\"    static threshold={distance_threshold:.3f} -> {n_clusters} clusters\")\n",
    "        else:\n",
    "            # Dynamic clustering\n",
    "            params = clustering_params or {}\n",
    "            cluster_assignments, cluster_info = determine_optimal_clustering(\n",
    "                linked, method=clustering_method, **params\n",
    "            )\n",
    "            n_clusters = cluster_info['n_clusters']\n",
    "            log(f\"    {clustering_method} method -> {n_clusters} clusters\")\n",
    "            if 'distance_threshold' in cluster_info:\n",
    "                log(f\"    dynamic threshold: {cluster_info['distance_threshold']:.3f}\")\n",
    "            if 'individual_estimates' in cluster_info:\n",
    "                log(f\"    individual method estimates: {cluster_info['individual_estimates']}\")\n",
    "        \n",
    "        log(f\"    clustering complete in {time.time() - t0:.2f}s\")\n",
    "        \n",
    "        clusters = defaultdict(list)\n",
    "        for i, cid in enumerate(cluster_assignments):\n",
    "            clusters[cid].append(label_list[i])\n",
    "        clusters_output[ssn_id] = clusters\n",
    "\n",
    "        # prepare coloring\n",
    "        _, label_to_color = _collect_cluster_colors(cluster_assignments, label_list, cmap_name=\"tab20\")\n",
    "\n",
    "        # draw circular tree\n",
    "        title = f\"SSN {ssn_id} gene neighborhoods\"\n",
    "        out_prefix = f\"SSN_{ssn_id}_gnn\"\n",
    "        _draw_circular_tree_with_clusters(\n",
    "            linkage_matrix=linked,\n",
    "            leaf_labels=label_list,\n",
    "            cluster_assignments=cluster_assignments,\n",
    "            label_to_cluster_color=label_to_color,\n",
    "            original_input_sequence_id=original_input_sequence_id,\n",
    "            labels_map=group_labels_map,\n",
    "            title=title,\n",
    "            out_prefix=out_prefix,\n",
    "            output_dir=output_dir,\n",
    "            output_formats=output_formats,\n",
    "            dpi=dpi,\n",
    "            show=not save_plots,\n",
    "            feature_matrix=mat,\n",
    "            inner_boundary=0.45,\n",
    "            consensus_threshold=0.2,\n",
    "            leaf_stretch_power=7,\n",
    "            gap_factor=0.1,\n",
    "        )\n",
    "\n",
    "        log(f\"--- SSN {ssn_id} done in {time.time() - group_start:.2f}s ---\")\n",
    "        del linked\n",
    "        gc.collect()\n",
    "\n",
    "    log(f\"\\nTotal runtime: {time.time() - t0_all:.2f}s\")\n",
    "    return clusters_output, final_neighborhood_labels_map, collapsed_groups_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d25c37c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GNN Clustering Report (2026-02-21 13:39:45) ---\n",
      "Database: D:\\Studium\\PhD\\Rotation\\1st_WetLab\\Enzyme_Homologe_Search\\CaMES\\CaMES_10kBlast_10e_50eEdge_noFilter_300AST_min900AA_withoutEgtD_withoutMethyltrans_10N\\39061_CaMES_10kBlast_10e_50eEdge_noFilter_300AST_min900AA_withoutEgtD_withoutMethyltrans_10N.sqlite\n",
      "Clustering method: combined\n",
      "  Dynamic clustering parameters: {'max_clusters': None, 'coherence_threshold': 0.7}\n",
      "Hit weight: 10\n",
      "Direct neighbor weight: 3\n",
      "Mode: SSN-separated\n",
      "Collapsing enabled:\n",
      "  Stage1 (core) thr: 0.0\n",
      "  Stage2 (full) thr: 0.2\n",
      "Distance parallelism: joblib (enabled)\n",
      "OMP_NUM_THREADS: 12\n",
      "Highlight accession: A0A7V4WV16 (color red)\n",
      "Plots -> gnn_cluster_plots_circular as ['pdf'], dpi=600\n",
      "Report -> gnn_cluster_plots_circular\\gnn_clustering_report_ssn_differentiated.txt\n",
      "----------------------------------------------------------------------\n",
      "Fetching hit gene data ...\n",
      "  Retrieved 37 hit genes\n",
      "Fetching neighbor data ...\n",
      "  Retrieved 800 neighbor rows\n",
      "Grouping neighbors by hit gene id ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Grouping neighbors:   0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for 37 neighborhoods ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Neighborhoods: 100%|| 37/37 [00:00<00:00, 939.78nh/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete.\n",
      "\n",
      "Raw SSN id distribution:\n",
      "  SSN 1: 35 neighborhoods\n",
      "  SSN 2: 1 neighborhoods\n",
      "  SSN 3: 1 neighborhoods\n",
      "------------------------------------------------------------\n",
      "[Collapsing]  Collapsing neighborhoods (core thr: 0.0, full thr: 0.2)\n",
      "[Collapsing]  Stage1: building core sparse matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collapsing]  Stage1: computing core Jaccard distances\n",
      "  Pre-computing feature sets for N=37 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done in 0.00s\n",
      "  Using parallel Jaccard (N=37, cores=12)\n",
      "[Collapsing]  Stage1: distance calc in 0.64s\n",
      "[Collapsing]  Stage1: 37 core groups (1.02s partial)\n",
      "[Collapsing]  Stage2: full-neighborhood collapsing\n",
      "[Collapsing]  No collapsing performed (1.52s).\n",
      "\n",
      "Processing 3 SSN groups (after filtering).\n",
      "  SSN ids: 1, 2, 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering SSN groups:   0%|          | 0/3 [00:00<?, ?group/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SSN 1 with 35 neighborhoods.\n",
      "\n",
      "--- SSN 1: 35 neighborhoods ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SSN 1: computing Jaccard distances ...\n",
      "  Pre-computing feature sets for N=35 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done in 0.01s\n",
      "  Using parallel Jaccard (N=35, cores=12)\n",
      "    distance calc in 1.55s\n",
      "  SSN 1: linkage ...\n",
      "    linkage in 0.00s\n",
      "  SSN 1: determining optimal clusters (method: combined) ...\n",
      "    combined method -> 6 clusters\n",
      "    individual method estimates: [13, 2, 6]\n",
      "    clustering complete in 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "meta NOT subset; don't know how to subset; dropped\n",
      "meta NOT subset; don't know how to subset; dropped\n",
      "Clustering SSN groups: 100%|| 3/3 [00:03<00:00,  1.25s/group]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SSN 1 done in 3.37s ---\n",
      "SSN 3: only 1 neighborhood(s); skipping clustering.\n",
      "SSN 2: only 1 neighborhood(s); skipping clustering.\n",
      "\n",
      "Total runtime: 6.47s\n",
      "\n",
      "--- Final clustering results ---\n",
      "\n",
      "### SSN 1 ###\n",
      "  Cluster 1: 4 neighborhoods\n",
      "    - Anaeromyxobacter diazotrophicus. | Acc: A0A7I9VRP5 (hit_id=BJTG01000010, nh=Anaeromyxobacter diazotrophicus._BJTG01000010)\n",
      "    - Calditrichota bacterium. | Acc: A0A3M2FG68 (hit_id=RFFS01000159, nh=Calditrichota bacterium._RFFS01000159)\n",
      "    - Desulfatitalea sp. BRH_c12. | Acc: A0A0F2R6L8 (hit_id=LADR01000008, nh=Desulfatitalea sp. BRH_c12._LADR01000008)\n",
      "    - Magnetococcus massalia (strain MO-1) | Acc: A0A1S7LH69 (hit_id=LO017727, nh=Magnetococcus massalia (strain MO-1)_LO017727)\n",
      "  Cluster 2: 27 neighborhoods\n",
      "    - Acidobacteriota bacterium. | Acc: A0A399XDS8 (hit_id=QEUP01000041, nh=Acidobacteriota bacterium._QEUP01000041)\n",
      "    - Actinomycetes bacterium. | Acc: A0A662DMY1 (hit_id=QMQC01000053, nh=Actinomycetes bacterium._QMQC01000053)\n",
      "    - Alteromonadaceae bacterium. | Acc: A0A2A5HFC2 (hit_id=NVXL01000214, nh=Alteromonadaceae bacterium._NVXL01000214)\n",
      "    - Bdellovibrionales bacterium GWB1_55_8. | Acc: A0A1F3T9T9 (hit_id=MEPR01000010, nh=Bdellovibrionales bacterium GWB1_55_8._MEPR01000010)\n",
      "    - Caldithrix abyssi. | Acc: A0A7V4WV16 (ORIGINAL) (hit_id=DRQG01000086, nh=Caldithrix abyssi._DRQG01000086)\n",
      "    - Calditrichota bacterium. | Acc: A0A849M3X8 (hit_id=JABFGF010000001, nh=Calditrichota bacterium._JABFGF010000001)\n",
      "    - Calditrichota bacterium. | Acc: A0A6H9L4H4 (hit_id=QQUE01000012, nh=Calditrichota bacterium._QQUE01000012)\n",
      "    - Calditrichota bacterium. | Acc: A0A3M1P214 (hit_id=RFIL01000057, nh=Calditrichota bacterium._RFIL01000057)\n",
      "    - Candidatus Jettenia ecosi. | Acc: A0A533Q600 (hit_id=SULG01000150, nh=Candidatus Jettenia ecosi._SULG01000150)\n",
      "    - Chromatiales bacterium 21-64-14. | Acc: A0A257SS02 (hit_id=NCBI01000051, nh=Chromatiales bacterium 21-64-14._NCBI01000051)\n",
      "    - Deltaproteobacteria bacterium. | Acc: A0A9D6GAE0 (hit_id=JACPRU010000070, nh=Deltaproteobacteria bacterium._JACPRU010000070)\n",
      "    - Desulfobulbaceae bacterium. | Acc: A0A2N6C8B1 (hit_id=PKTW01000072, nh=Desulfobulbaceae bacterium._PKTW01000072)\n",
      "    - Gammaproteobacteria bacterium. | Acc: A0A7C1TU51 (hit_id=DRJN01000160, nh=Gammaproteobacteria bacterium._DRJN01000160)\n",
      "    - Gammaproteobacteria bacterium. | Acc: A0A7C1TE37 (hit_id=DRKR01000022, nh=Gammaproteobacteria bacterium._DRKR01000022)\n",
      "    - Gemmatimonadales bacterium. | Acc: A0A9E5PG39 (hit_id=WVYX01000149, nh=Gemmatimonadales bacterium._WVYX01000149)\n",
      "    - Gemmatimonadota bacterium. | Acc: A0AA42Y149 (hit_id=JAOTVV010000005, nh=Gemmatimonadota bacterium._JAOTVV010000005)\n",
      "    - Methanomicrobiales archaeon. | Acc: A0A7K4BI03 (hit_id=JAAZII010000286, nh=Methanomicrobiales archaeon._JAAZII010000286)\n",
      "    - Methanomicrobiales archaeon. | Acc: A0A7K4DD47 (hit_id=JAAZOW010000012, nh=Methanomicrobiales archaeon._JAAZOW010000012)\n",
      "    - Nitrospirae bacterium GWD2_57_9. | Acc: A0A1G1G530 (hit_id=MHED01000119, nh=Nitrospirae bacterium GWD2_57_9._MHED01000119)\n",
      "    - Planctomycetota bacterium. | Acc: A0A523NF38 (hit_id=SMVR01000114, nh=Planctomycetota bacterium._SMVR01000114)\n",
      "    - Polaromonas sp. | Acc: A0A3C0YC61 (hit_id=DMKL01000304, nh=Polaromonas sp._DMKL01000304)\n",
      "    - Tectimicrobiota bacterium. | Acc: A0A932ML91 (hit_id=JACPUR010000013, nh=Tectimicrobiota bacterium._JACPUR010000013)\n",
      "    - Thiorhodococcus mannitoliphagus. | Acc: A0A6P1DUA6 (hit_id=JAAIJR010000030, nh=Thiorhodococcus mannitoliphagus._JAAIJR010000030)\n",
      "    - Thiorhodococcus minor. | Acc: A0A6M0JW57 (hit_id=JAAIJQ010000009, nh=Thiorhodococcus minor._JAAIJQ010000009)\n",
      "    - bacterium BMS3Abin03. | Acc: A0A2H6ED39 (hit_id=BDSV01000107, nh=bacterium BMS3Abin03._BDSV01000107)\n",
      "    - hydrothermal vent metagenome. | Acc: A0A3B0YYM1 (hit_id=UOFM01000067, nh=hydrothermal vent metagenome._UOFM01000067)\n",
      "    - hydrothermal vent metagenome. | Acc: A0A3B0Z7Q1 (hit_id=UOFM01000459, nh=hydrothermal vent metagenome._UOFM01000459)\n",
      "  Cluster 3: 1 neighborhoods\n",
      "    - bacterium BMS3Bbin12. | Acc: A0A2H6JEA0 (hit_id=BDTV01000069, nh=bacterium BMS3Bbin12._BDTV01000069)\n",
      "  Cluster 4: 1 neighborhoods\n",
      "    - Thioflavicoccus mobilis 8321. | Acc: L0GVW7 (hit_id=CP003051, nh=Thioflavicoccus mobilis 8321._CP003051)\n",
      "  Cluster 5: 1 neighborhoods\n",
      "    - Thiohalomonas denitrificans. | Acc: A0A1G5QK56 (hit_id=FMWD01000006, nh=Thiohalomonas denitrificans._FMWD01000006)\n",
      "  Cluster 6: 1 neighborhoods\n",
      "    - Candidatus Methanophaga sp. ANME-1 ERB7. | Acc: A0A7G9ZD04 (hit_id=MT631715, nh=Candidatus Methanophaga sp. ANME-1 ERB7._MT631715)\n",
      "  ------------------------------\n",
      "\n",
      "### SSN 2 ###\n",
      "  Cluster 1: 1 neighborhoods\n",
      "    - Gammaproteobacteria bacterium. | Acc: A0A661CEM9 (hit_id=QNER01001088, nh=Gammaproteobacteria bacterium._QNER01001088)\n",
      "  ------------------------------\n",
      "\n",
      "### SSN 3 ###\n",
      "  Cluster 1: 1 neighborhoods\n",
      "    - Thermoprotei archaeon. | Acc: A0A7J2SZI2 (hit_id=DSCF01000035, nh=Thermoprotei archaeon._DSCF01000035)\n",
      "  ------------------------------\n",
      "\n",
      "--- Report end ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# High-level driver / example usage\n",
    "# ----------------------------------------------------------------------\n",
    "SQLITE_DB_PATH = r\"D:\\Studium\\PhD\\Rotation\\1st_WetLab\\Enzyme_Homologe_Search\\CaMES\\CaMES_10kBlast_10e_50eEdge_noFilter_300AST_min900AA_withoutEgtD_withoutMethyltrans_10N\\39061_CaMES_10kBlast_10e_50eEdge_noFilter_300AST_min900AA_withoutEgtD_withoutMethyltrans_10N.sqlite\"\n",
    "ORIGINAL_INPUT_SEQUENCE_ID = \"A0A7V4WV16\"  # or None\n",
    "DIFFERENTIATE_BY_SSN_CLUSTER = True  # Whether to cluster neighborhoods separately by SSN cluster\n",
    "\n",
    "# --- Clustering method configuration ---\n",
    "# Choose: 'gap', 'inconsistency', 'elbow', 'lifetime', 'topology', 'maxclust', 'combined', or 'static'\n",
    "# CLUSTERING_METHOD = 'topology'  # RECOMMENDED: ensures visually coherent clusters matching tree structure\n",
    "# CLUSTERING_PARAMS = {\n",
    "#     'max_clusters': None,        # Maximum clusters (None = auto, uses sqrt(n))\n",
    "#     'coherence_threshold': 0.7   # How coherent subtrees should be (0-1, higher = more coherent)\n",
    "# }\n",
    "\n",
    "# Alternative methods:\n",
    "# CLUSTERING_METHOD = 'gap'  # Fast, good for well-separated clusters\n",
    "# CLUSTERING_PARAMS = {'min_clusters': 2, 'min_gap_ratio': 1.5}\n",
    "\n",
    "# CLUSTERING_METHOD = 'maxclust'  # Force specific number of clusters\n",
    "# CLUSTERING_PARAMS = {'target_clusters': 5}\n",
    "\n",
    "CLUSTERING_METHOD = 'combined'  # Most robust (consensus of multiple methods)\n",
    "\n",
    "# For legacy behavior with fixed threshold:\n",
    "# CLUSTERING_METHOD = 'static'\n",
    "\n",
    "chosen_distance_threshold = 0.6  # Only used if CLUSTERING_METHOD = 'static'\n",
    "PARALLELIZE_PDIST_ENABLED = True\n",
    "\n",
    "COLLAPSE_IDENTICAL_NEIGHBORHOODS_ACTIVE = True\n",
    "COLLAPSE_CORE_SIMILARITY_THRESHOLD_ACTIVE = 0.0\n",
    "COLLAPSE_FULL_NEIGHBORHOOD_SIMILARITY_THRESHOLD_ACTIVE = 0.2\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "report_suffix = \"_ssn_differentiated\" if DIFFERENTIATE_BY_SSN_CLUSTER else \"_all_neighborhoods\"\n",
    "report_filename = f\"{REPORT_FILENAME_BASE}{report_suffix}.txt\"\n",
    "report_path = os.path.join(OUTPUT_DIR, report_filename)\n",
    "\n",
    "with open(report_path, \"w\") as report_file:\n",
    "    def log_top(msg):\n",
    "        print(msg)\n",
    "        report_file.write(msg + \"\\n\")\n",
    "\n",
    "    log_top(\n",
    "        f\"\\n--- GNN Clustering Report \"\n",
    "        f\"({datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}) ---\"\n",
    "    )\n",
    "    log_top(f\"Database: {SQLITE_DB_PATH}\")\n",
    "    log_top(f\"Clustering method: {CLUSTERING_METHOD}\")\n",
    "    if CLUSTERING_METHOD == 'static':\n",
    "        log_top(f\"  Static distance threshold: {chosen_distance_threshold}\")\n",
    "    else:\n",
    "        log_top(f\"  Dynamic clustering parameters: {CLUSTERING_PARAMS}\")\n",
    "    log_top(f\"Hit weight: {HIT_GENE_WEIGHT_FACTOR}\")\n",
    "    log_top(f\"Direct neighbor weight: {DIRECT_NEIGHBOR_WEIGHT_FACTOR}\")\n",
    "    log_top(\n",
    "        \"Mode: \"\n",
    "        + (\"SSN-separated\" if DIFFERENTIATE_BY_SSN_CLUSTER else \"all neighborhoods together\")\n",
    "    )\n",
    "    if COLLAPSE_IDENTICAL_NEIGHBORHOODS_ACTIVE:\n",
    "        log_top(\"Collapsing enabled:\")\n",
    "        log_top(f\"  Stage1 (core) thr: {COLLAPSE_CORE_SIMILARITY_THRESHOLD_ACTIVE}\")\n",
    "        log_top(f\"  Stage2 (full) thr: {COLLAPSE_FULL_NEIGHBORHOOD_SIMILARITY_THRESHOLD_ACTIVE}\")\n",
    "    else:\n",
    "        log_top(\"Collapsing disabled.\")\n",
    "\n",
    "    log_top(\n",
    "        \"Distance parallelism: \"\n",
    "        + (\"joblib (enabled)\" if PARALLELIZE_PDIST_ENABLED else \"sequential\")\n",
    "    )\n",
    "    log_top(f\"OMP_NUM_THREADS: {os.environ.get('OMP_NUM_THREADS', 'not set')}\")\n",
    "    if ORIGINAL_INPUT_SEQUENCE_ID:\n",
    "        log_top(\n",
    "            f\"Highlight accession: {ORIGINAL_INPUT_SEQUENCE_ID} \"\n",
    "            f\"(color {HIGHLIGHT_COLOR})\"\n",
    "        )\n",
    "    else:\n",
    "        log_top(\"No highlight accession set.\")\n",
    "    log_top(f\"Plots -> {OUTPUT_DIR} as {OUTPUT_FORMATS}, dpi={DPI}\")\n",
    "    log_top(f\"Report -> {report_path}\")\n",
    "    log_top(\"-\" * 70)\n",
    "\n",
    "    clusters_by_ssn, final_labels_map, collapsed_groups_report = cluster_gene_neighborhoods_from_sqlite(\n",
    "        db_path=SQLITE_DB_PATH,\n",
    "        genes_table=GENES_TABLE,\n",
    "        neighbors_table=NEIGHBORS_TABLE,\n",
    "        differentiate_by_ssn_cluster=DIFFERENTIATE_BY_SSN_CLUSTER,\n",
    "        ssn_cluster_value_to_filter=DEFAULT_SSN_CLUSTER_VALUE_TO_FILTER,\n",
    "        collapse_identical_neighborhoods=COLLAPSE_IDENTICAL_NEIGHBORHOODS_ACTIVE,\n",
    "        collapse_core_similarity_threshold=COLLAPSE_CORE_SIMILARITY_THRESHOLD_ACTIVE,\n",
    "        collapse_full_neighborhood_similarity_threshold=COLLAPSE_FULL_NEIGHBORHOOD_SIMILARITY_THRESHOLD_ACTIVE,\n",
    "        original_input_sequence_id=ORIGINAL_INPUT_SEQUENCE_ID,\n",
    "        distance_threshold=chosen_distance_threshold,\n",
    "        clustering_method=CLUSTERING_METHOD,\n",
    "        clustering_params=CLUSTERING_PARAMS,\n",
    "        save_plots=SAVE_PLOTS,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        output_formats=OUTPUT_FORMATS,\n",
    "        dpi=DPI,\n",
    "        report_file_handle=report_file,\n",
    "        parallelize_pdist=PARALLELIZE_PDIST_ENABLED,\n",
    "    )\n",
    "\n",
    "    if clusters_by_ssn:\n",
    "        log_top(\"\\n--- Final clustering results ---\")\n",
    "        for ssn_id, clusters_in_ssn in sorted(clusters_by_ssn.items(), key=lambda x: str(x[0])):\n",
    "            log_top(f\"\\n### SSN {ssn_id} ###\")\n",
    "            if not clusters_in_ssn:\n",
    "                log_top(\"  No clusters.\")\n",
    "                continue\n",
    "            for cid, nh_list in sorted(clusters_in_ssn.items()):\n",
    "                log_top(f\"  Cluster {cid}: {len(nh_list)} neighborhoods\")\n",
    "                for nh in nh_list:\n",
    "                    org, hit_id, _, acc, collapsed_info = final_labels_map.get(\n",
    "                        nh, (\"UNKNOWN\", \"UNKNOWN\", None, \"UNKNOWN\", None)\n",
    "                    )\n",
    "                    highlight = \" (ORIGINAL)\" if ORIGINAL_INPUT_SEQUENCE_ID and acc == ORIGINAL_INPUT_SEQUENCE_ID else \"\"\n",
    "                    collapsed_suffix = \"\"\n",
    "                    if collapsed_info:\n",
    "                        count, code = collapsed_info\n",
    "                        collapsed_suffix = f\" (Collapsed: {count}, code: {code})\"\n",
    "                    log_top(\n",
    "                        f\"    - {org} | Acc: {acc}{highlight}{collapsed_suffix} \"\n",
    "                        f\"(hit_id={hit_id}, nh={nh})\"\n",
    "                    )\n",
    "            log_top(\"  \" + \"-\" * 30)\n",
    "\n",
    "        if collapsed_groups_report:\n",
    "            log_top(\"\\n--- Collapsed neighborhood groups ---\")\n",
    "            for code, info in sorted(collapsed_groups_report.items()):\n",
    "                log_top(\n",
    "                    f\"  Group {code}: rep={info['representative']} \"\n",
    "                    f\"(n={info['count']})\"\n",
    "                )\n",
    "                for nh in info[\"members\"]:\n",
    "                    org, hit_id, _, acc, _ = final_labels_map.get(\n",
    "                        nh, (\"UNKNOWN\", \"UNKNOWN\", None, \"UNKNOWN\", None)\n",
    "                    )\n",
    "                    log_top(\n",
    "                        f\"    - {org} | Acc: {acc} (hit_id={hit_id}, nh={nh})\"\n",
    "                    )\n",
    "            log_top(\"-\" * 60)\n",
    "    else:\n",
    "        log_top(\"\\nNo clusters formed at all.\")\n",
    "\n",
    "    log_top(\"\\n--- Report end ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf64922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
